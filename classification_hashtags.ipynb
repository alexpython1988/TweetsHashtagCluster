{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets classification by its hashtags and no labeled tweets hashtag predication\n",
    "\n",
    "\n",
    "** Using top n hashtags as label to build a supervised model for tweets classification and hashtag predication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load packages and modeling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9338521723921780621\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow backend\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.8\n",
      "tensorflow\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import *\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "print(keras.__version__)\n",
    "print(keras.backend.backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bcolz\n",
    "import pickle\n",
    "\n",
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "    \n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]\n",
    "\n",
    "def save_dict(fname, dictionary):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(dictionary, f)\n",
    "\n",
    "def load_dict(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hashtag_label': [1, 5],\n",
       " 'hashtags': ['hpv', 'vaccin'],\n",
       " 'id': '418263863772327936',\n",
       " 'orignal_hashtags': ['#hpv', '#vaccine'],\n",
       " 'raw': 'rt @cdcstd: #hpv vax coverage could be 93% if doctors gave hpv #vaccine each time a preteen/teen got any other vaccine&gt; http://t.co/xxryga5â€¦',\n",
       " 'text': 'rt : hpv vax coverage could be 93% if doctors gave hpv vaccine each time a preteen / teen got any other vaccine>',\n",
       " 'words': ['rt',\n",
       "  ':',\n",
       "  'hpv',\n",
       "  'vax',\n",
       "  'coverage',\n",
       "  'could',\n",
       "  'be',\n",
       "  '93',\n",
       "  '%',\n",
       "  'if',\n",
       "  'doctors',\n",
       "  'gave',\n",
       "  'hpv',\n",
       "  'vaccine',\n",
       "  'each',\n",
       "  'time',\n",
       "  'a',\n",
       "  'preteen',\n",
       "  '/',\n",
       "  'teen',\n",
       "  'got',\n",
       "  'any',\n",
       "  'other',\n",
       "  'vaccine',\n",
       "  '>']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load tweets data\n",
    "import json\n",
    "tweets_file = \"temp/tweets4classification.json\"\n",
    "with open(tweets_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    modeling_data = json.load(f)\n",
    "modeling_data['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load processed word enbeddings\n",
    "path = 'wordsenbeddings/'\n",
    "res_path = path + 'results/'\n",
    "\n",
    "def load_vectors(name):\n",
    "    loc = res_path + name\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_glove(name, dim):\n",
    "    with open(path+ 'glove.' + name + '.txt', 'r', encoding=\"utf-8\") as f:\n",
    "        vecs = []\n",
    "        words = []\n",
    "        \n",
    "        for i, line in enumerate(f):\n",
    "            d = line.split()\n",
    "            word = d[0]\n",
    "            vec = np.array(d[1:], dtype=np.float32)\n",
    "            if (len(d) == dim): # this is space\n",
    "                word = ' '\n",
    "                vec = np.array(d, dtype=np.float32)\n",
    "            \n",
    "            words.append(word)            \n",
    "            vecs.append(vec)\n",
    "\n",
    "        wordidx = {o:i for i,o in enumerate(words)}\n",
    "        save_array(res_path+name+'.dat', vecs)\n",
    "        pickle.dump(words, open(res_path+name+'_words.pkl','wb'))\n",
    "        pickle.dump(wordidx, open(res_path+name+'_idx.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_glove('twitter.27B.200d', 200)\n",
    "get_glove('twitter.27B.25d', 25)\n",
    "get_glove('twitter.27B.50d', 50)\n",
    "get_glove('twitter.27B.100d', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare train and test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ list(['rt', ':', 'hpv', 'vax', 'coverage', 'could', 'be', '93', '%', 'if', 'doctors', 'gave', 'hpv', 'vaccine', 'each', 'time', 'a', 'preteen', '/', 'teen', 'got', 'any', 'other', 'vaccine', '>'])\n",
      " list(['rt', ':', 'hpv', 'vax', 'coverage', 'could', 'be', '93', '%', 'if', 'doctors', 'gave', 'hpv', 'vaccine', 'each', 'time', 'a', 'preteen', '/', 'teen', 'got', 'any', 'other', 'vaccine', '...', '.'])]\n",
      "[list([1, 5]) list([1, 5])]\n",
      "81049\n",
      "81049\n"
     ]
    }
   ],
   "source": [
    "data = np.asarray([each['words'] for each in modeling_data['data']])\n",
    "label = np.asarray([each['hashtag_label'] for each in modeling_data['data']])\n",
    "\n",
    "print(data[:2])\n",
    "print(label[:2])\n",
    "print(len(data))\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "categorical_label = list(map(lambda x: to_categorical(x, num_classes=modeling_data['categorical_num']), label))\n",
    "\n",
    "print(len(categorical_label))\n",
    "categorical_label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_label[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, categorical_label, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ list(['rt', ':', 'hpv', 'fact', ':', 'hpv', 'is', 'the', 'primary', 'cause', 'of', 'cervicalcancer', ',', 'certain', 'types', 'of', 'head', '&', 'neck', 'cancer', ',', 'in', 'addition', 'to', 'several', 'rare', 'cance'])\n",
      " list(['rt', ':', 'study', ':', 'hpv', 'vaccine', 'linked', 'to', 'premature', 'menopause', 'in', 'young', 'girls'])]\n",
      "[ list(['two', 'uk', 'girls', 'left', 'paralyzed', 'after', 'hpv', 'jabs', '.', 'authorities', 'still', 'claim', 'it', \"'s\", 'coincidence', '.'])\n",
      " list(['cervicalcancer', 'deaths', 'have', 'decreased', 'dramatically', 'over', 'the', 'past', '40', 'years', ',', 'mostly', 'due', 'to', 'increased', 'screening', '.'])]\n",
      "[array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]]), array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]), array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]])]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:2])\n",
    "print(X_test[:2])\n",
    "print(y_train[:2])\n",
    "print(y_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create words dictionary for the data\n",
    "from functools import reduce\n",
    "\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "all_tokens = list(reduce(lambda x, y: x + y, [l['words'] for l in modeling_data['data']]))\n",
    "print(all_tokens[:5])\n",
    "\n",
    "for token in all_tokens:\n",
    "    frequency[token] += 1\n",
    "    \n",
    "dictionary = sorted(frequency.items(), key=lambda x:x[1], reverse=True)\n",
    "if (not vocab_size):\n",
    "    vocab_size = len(dictionary)\n",
    "dictionary = [k for k,v in dictionary[:vocab_size]]\n",
    "print(dictionary[:10])\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dict('model/dict.dd', dictionary)\n",
    "dictionary = load_dict('model/dict.dd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word to index\n",
    "X_train_f = [[dictionary.index(word) if word in dictionary else -1 for word in doc] for doc in X_train]\n",
    "X_test_f = [[dictionary.index(word) if word in dictionary else -1 for word in doc] for doc in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding words using GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
