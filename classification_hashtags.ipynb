{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets classification by its hashtags and no labeled tweets hashtag predication\n",
    "\n",
    "\n",
    "**Using top n hashtags as label to build a supervised model for tweets classification and hashtag predication**\n",
    "\n",
    "[1.1 load data](1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='1.1'> load packages and modeling data </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12881373835584278877\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow backend\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n",
      "tensorflow\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import *\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "print(keras.__version__)\n",
    "print(keras.backend.backend())\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bcolz\n",
    "import pickle\n",
    "\n",
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "    \n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]\n",
    "\n",
    "def save_dict(fname, dictionary):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(dictionary, f)\n",
    "\n",
    "def load_dict(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hashtag_label': [1, 5],\n",
       " 'hashtags': ['hpv', 'vaccin'],\n",
       " 'id': '418263863772327936',\n",
       " 'orignal_hashtags': ['#hpv', '#vaccine'],\n",
       " 'raw': 'rt @cdcstd: #hpv vax coverage could be 93% if doctors gave hpv #vaccine each time a preteen/teen got any other vaccine&gt; http://t.co/xxryga5â€¦',\n",
       " 'text': 'rt : hpv vax coverage could be 93% if doctors gave hpv vaccine each time a preteen / teen got any other vaccine>',\n",
       " 'words': ['rt',\n",
       "  ':',\n",
       "  'hpv',\n",
       "  'vax',\n",
       "  'coverage',\n",
       "  'could',\n",
       "  'be',\n",
       "  '93',\n",
       "  '%',\n",
       "  'if',\n",
       "  'doctors',\n",
       "  'gave',\n",
       "  'hpv',\n",
       "  'vaccine',\n",
       "  'each',\n",
       "  'time',\n",
       "  'a',\n",
       "  'preteen',\n",
       "  '/',\n",
       "  'teen',\n",
       "  'got',\n",
       "  'any',\n",
       "  'other',\n",
       "  'vaccine',\n",
       "  '>']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load tweets data\n",
    "import json\n",
    "tweets_file = \"temp/tweets4classification.json\"\n",
    "with open(tweets_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    modeling_data = json.load(f)\n",
    "modeling_data['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load processed word enbeddings\n",
    "path = 'wordsenbeddings/'\n",
    "res_path = path + 'results/'\n",
    "\n",
    "def load_vectors(name):\n",
    "    loc = res_path + name\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_glove(name, dim):\n",
    "    with open(path+ 'glove.' + name + '.txt', 'r', encoding=\"utf-8\") as f:\n",
    "        vecs = []\n",
    "        words = []\n",
    "        \n",
    "        for i, line in enumerate(f):\n",
    "            d = line.split()\n",
    "            word = d[0]\n",
    "            vec = np.array(d[1:], dtype=np.float32)\n",
    "            if (len(d) == dim): # this is space\n",
    "                word = ' '\n",
    "                vec = np.array(d, dtype=np.float32)\n",
    "            \n",
    "            words.append(word)            \n",
    "            vecs.append(vec)\n",
    "\n",
    "        wordidx = {o:i for i,o in enumerate(words)}\n",
    "        save_array(res_path+name+'.dat', vecs)\n",
    "        pickle.dump(words, open(res_path+name+'_words.pkl','wb'))\n",
    "        pickle.dump(wordidx, open(res_path+name+'_idx.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_glove('twitter.27B.200d', 200)\n",
    "get_glove('twitter.27B.25d', 25)\n",
    "get_glove('twitter.27B.50d', 50)\n",
    "get_glove('twitter.27B.100d', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare train and test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ list(['rt', ':', 'hpv', 'vax', 'coverage', 'could', 'be', '93', '%', 'if', 'doctors', 'gave', 'hpv', 'vaccine', 'each', 'time', 'a', 'preteen', '/', 'teen', 'got', 'any', 'other', 'vaccine', '>'])\n",
      " list(['rt', ':', 'hpv', 'vax', 'coverage', 'could', 'be', '93', '%', 'if', 'doctors', 'gave', 'hpv', 'vaccine', 'each', 'time', 'a', 'preteen', '/', 'teen', 'got', 'any', 'other', 'vaccine', '...', '.'])]\n",
      "[list([1, 5]) list([1, 5])]\n",
      "81049\n",
      "81049\n"
     ]
    }
   ],
   "source": [
    "data = np.asarray([each['words'] for each in modeling_data['data']])\n",
    "label = np.asarray([each['hashtag_label'] for each in modeling_data['data']])\n",
    "\n",
    "print(data[:2])\n",
    "print(label[:2])\n",
    "print(len(data))\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flat_labels(labels):\n",
    "    flatted = []\n",
    "    l = modeling_data['categorical_num']\n",
    "    for label in labels:\n",
    "        m = [0.] * l\n",
    "        for each in label:\n",
    "            m = list(map(lambda x: x[0] or x[1], zip(m, each)))\n",
    "        flatted.append(m)\n",
    "    return np.asarray(flatted)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "categorical_label = list(map(lambda x: to_categorical(x, num_classes=modeling_data['categorical_num']), label))\n",
    "\n",
    "categorical_label_flatted = flat_labels(categorical_label)\n",
    "\n",
    "print(len(categorical_label))\n",
    "categorical_label_flatted[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_0, X_test, y_train_0, y_test = train_test_split(data, categorical_label_flatted, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ list(['rt', ':', 'new', 'infographic', 'on', 'how', 'most', 'cases', 'of', 'cervicalcancer', 'can', 'be', 'prevented', 'w', '/', 'tests', '&', 'hpv', 'vaccine', '.', 'vitalsigns'])\n",
      " list(['check', 'out', 'the', 'gci', 'team', \"'s\", 'newest', 'publication', 'on', 'hpv', 'vaccine', 'implementation', 'for', 'cancer', 'prevention', 'in', 'latinamerica', '!'])]\n",
      "[ list(['two', 'uk', 'girls', 'left', 'paralyzed', 'after', 'hpv', 'jabs', '.', 'authorities', 'still', 'claim', 'it', \"'s\", 'coincidence', '.'])\n",
      " list(['cervicalcancer', 'deaths', 'have', 'decreased', 'dramatically', 'over', 'the', 'past', '40', 'years', ',', 'mostly', 'due', 'to', 'increased', 'screening', '.'])]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  1.]\n",
      " [ 0.  1.  1.  0.  0.  1.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_0[:2])\n",
    "print(X_test[:2])\n",
    "print(y_train_0[:10])\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save 5% train samples to predication task\n",
    "cut = int(len(X_train_0) * 0.95)\n",
    "X_train = X_train_0[:cut]\n",
    "y_train = y_train_0[:cut]\n",
    "X_pred = X_train_0[cut:]\n",
    "y_pred = y_train_0[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!!!do not run this, run the third one below this to directly load the dictionary\n",
    "#create words dictionary for the data\n",
    "from functools import reduce\n",
    "\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "all_tokens = list(reduce(lambda x, y: x + y, [l['words'] for l in modeling_data['data']]))\n",
    "print(all_tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(all_tokens))\n",
    "\n",
    "for token in all_tokens:\n",
    "    frequency[token] += 1\n",
    "    \n",
    "dictionary = sorted(frequency.items(), key=lambda x:x[1], reverse=True)\n",
    "dictionary = [k for k,v in dictionary[:vocab_size]]\n",
    "print(dictionary[:10])\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dict('model/dict.dd', dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':',\n",
       " 'hpv',\n",
       " 'rt',\n",
       " 'vaccine',\n",
       " '.',\n",
       " 'the',\n",
       " ',',\n",
       " 'of',\n",
       " 'to',\n",
       " 'cancer',\n",
       " 'in',\n",
       " 'gardasil',\n",
       " 'for',\n",
       " 'cervicalcancer',\n",
       " 'a',\n",
       " '&',\n",
       " 'and',\n",
       " 'is',\n",
       " '!',\n",
       " 'cervical']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 0\n",
    "dictionary = load_dict('model/dict.dd')\n",
    "if (not vocab_size):\n",
    "    vocab_size = len(dictionary)\n",
    "dictionary[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word to index\n",
    "X_train_f = [[dictionary.index(word) if word in dictionary else -1 for word in doc] for doc in X_train]\n",
    "X_test_f = [[dictionary.index(word) if word in dictionary else -1 for word in doc] for doc in X_test]\n",
    "X_pred_f = [[dictionary.index(word) if word in dictionary else -1 for word in doc] for doc in X_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 0, 46, 781, 22, 74, 100, 121, 7, 13, 35, 55, 352, 54, 40, 162, 15, 1, 3, 4, 1126], [364, 93, 5, 10430, 1019, 20, 1770, 3765, 22, 1, 3, 1637, 12, 9, 61, 10, 8726, 18], [1159, 49, 251, 22, 97, 4382, 12, 448, 3863, 8, 20, 1, 25, 493, 18, 400, 13], [2, 0, 122, 312, 90, 662, 57, 8, 369, 33, 13, 4, 529, 26, 967, 16, 1777, 831, 8, 39, 365, 12, 1], [2, 0, 27, 0, 11, 131, 19, 328, 10, 31, 244, 54, 40, 1, 51]]\n",
      "[[210, 338, 41, 215, 165, 34, 1, 471, 4, 459, 182, 409, 29, 20, 411, 4], [13, 180, 57, 1813, 2238, 296, 5, 1477, 1003, 134, 6, 3875, 616, 8, 448, 67, 4], [2, 0, 27, 0, 14, 262, 7, 346, 362, 10, 14, 509, 110, 118, 250, 34, 168, 1, 3], [726, 12, 8803, 423, 23, 155, 15, 281, 223, 397, 8, 1, 0, 492, 287], [97, 1, 16, 9, 153, 2965, 3326, 3551, 22, 2585, 193, 18, 1077, 125, 8, 159, 0]]\n",
      "[[3, 1413, 264, 41, 177, 1, 68, 32, 757, 37, 697], [2, 0, 4, 84, 981, 18, 11, 3379, 6, 226, 2928, 10352, 1760, 81, 10353, 6, 4817, 19993, 4], [2, 0, 30, 24, 478, 11, 11792, 21], [1, 25, 68, 569, 145, 917, 41, 10, 2948, 1620, 45], [2, 0, 1026, 0, 2602, 1592, 16, 1282, 4158, 7, 19, 9, 422, 33, 1780]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_f[:5])\n",
    "print(X_test_f[:5])\n",
    "print(X_pred_f[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding words using GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 2, 17.012127213987693)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array([len(doc) for doc in X_train_f])\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 39\n",
    "embedding_dim = 200\n",
    "vecs, words, wordidx = load_vectors('twitter.27B.%dd'%(embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_u = sequence.pad_sequences(X_train_f, maxlen=seq_len)\n",
    "X_test_u = sequence.pad_sequences(X_test_f, maxlen=seq_len)\n",
    "X_pred_u = sequence.pad_sequences(X_pred_f, maxlen=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     2     0    46   781    22    74\n",
      "    100   121     7    13    35    55   352    54    40   162    15     1\n",
      "      3     4  1126]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0   364    93     5\n",
      "  10430  1019    20  1770  3765    22     1     3  1637    12     9    61\n",
      "     10  8726    18]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0  1159    49\n",
      "    251    22    97  4382    12   448  3863     8    20     1    25   493\n",
      "     18   400    13]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     2     0   122   312    90   662    57     8\n",
      "    369    33    13     4   529    26   967    16  1777   831     8    39\n",
      "    365    12     1]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      2     0    27     0    11   131    19   328    10    31   244    54\n",
      "     40     1    51]]\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0  210  338   41  215  165\n",
      "    34    1  471    4  459  182  409   29   20  411    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0   13  180   57 1813 2238  296\n",
      "     5 1477 1003  134    6 3875  616    8  448   67    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    2    0   27    0   14  262    7  346\n",
      "   362   10   14  509  110  118  250   34  168    1    3]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0  726   12 8803  423\n",
      "    23  155   15  281  223  397    8    1    0  492  287]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0   97    1   16    9  153 2965\n",
      "  3326 3551   22 2585  193   18 1077  125    8  159    0]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     3  1413   264    41   177     1    68    32\n",
      "    757    37   697]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     2     0     4    84\n",
      "    981    18    11  3379     6   226  2928 10352  1760    81 10353     6\n",
      "   4817 19993     4]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     2     0    30    24   478\n",
      "     11 11792    21]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     1    25    68   569   145   917    41    10\n",
      "   2948  1620    45]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      2     0  1026     0  2602  1592    16  1282  4158     7    19     9\n",
      "    422    33  1780]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_u[:5])\n",
    "print(X_test_u[:5])\n",
    "print(X_pred_u[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embedding(dictionary):\n",
    "    print(vecs.shape)\n",
    "    n_fact = vecs.shape[1]\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "\n",
    "    for i, word in enumerate(dictionary):\n",
    "        #if word:# and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
    "        src_idx = wordidx[word] if word in wordidx else 0\n",
    "        \n",
    "        if src_idx:\n",
    "            emb[i] = vecs[src_idx]\n",
    "        else:\n",
    "            # If we can't find the word in glove, randomly initialize\n",
    "            emb[i] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "\n",
    "    # This is our \"rare word\" id - we want to randomly initialize\n",
    "    emb[-1] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "    emb/=3\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1193517, 200)\n"
     ]
    }
   ],
   "source": [
    "embedding = create_embedding(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create NN mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi-label model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 39, 200)           4661400   \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 39, 200)           200200    \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 39, 200)           200200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 20, 200)           0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 20, 200)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 11)                44011     \n",
      "=================================================================\n",
      "Total params: 5,105,811\n",
      "Trainable params: 5,105,811\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#pure cnn model\n",
    "dth = 0.5\n",
    "filter_num = 5\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#embedding layer\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=seq_len, weights=[embedding], trainable=True))\n",
    "# model.add(Dropout(dth))\n",
    "\n",
    "#hidden layer\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "model.add(Dropout(dth))\n",
    "# model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "# model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "# model.add(Dropout(dth))\n",
    "# model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "# model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "# model.add(Dropout(dth))\n",
    "\n",
    "#output layer\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(dth))\n",
    "model.add(Dense(modeling_data['categorical_num'], activation='softmax'))\n",
    "\n",
    "#compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61597 samples, validate on 16210 samples\n",
      "Epoch 1/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.4946 - acc: 0.7398Epoch 00001: val_loss improved from inf to 1.25936, saving model to model/cnn_hashtagcls_emb200_weights2cnn+max+drop.h5\n",
      "61597/61597 [==============================] - 356s 6ms/step - loss: 1.4944 - acc: 0.7398 - val_loss: 1.2594 - val_acc: 0.6869\n",
      "Epoch 2/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.2199 - acc: 0.7627Epoch 00002: val_loss improved from 1.25936 to 1.21467, saving model to model/cnn_hashtagcls_emb200_weights2cnn+max+drop.h5\n",
      "61597/61597 [==============================] - 344s 6ms/step - loss: 1.2197 - acc: 0.7627 - val_loss: 1.2147 - val_acc: 0.7394\n",
      "Epoch 3/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1530 - acc: 0.7658Epoch 00003: val_loss did not improve\n",
      "61597/61597 [==============================] - 343s 6ms/step - loss: 1.1533 - acc: 0.7659 - val_loss: 1.2184 - val_acc: 0.7535\n",
      "Epoch 4/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1183 - acc: 0.7690Epoch 00004: val_loss did not improve\n",
      "61597/61597 [==============================] - 345s 6ms/step - loss: 1.1184 - acc: 0.7690 - val_loss: 1.2150 - val_acc: 0.7804\n",
      "Epoch 5/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0952 - acc: 0.7695Epoch 00005: val_loss improved from 1.21467 to 1.21418, saving model to model/cnn_hashtagcls_emb200_weights2cnn+max+drop.h5\n",
      "61597/61597 [==============================] - 343s 6ms/step - loss: 1.0952 - acc: 0.7695 - val_loss: 1.2142 - val_acc: 0.7131\n",
      "Epoch 6/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0811 - acc: 0.7680Epoch 00006: val_loss improved from 1.21418 to 1.20811, saving model to model/cnn_hashtagcls_emb200_weights2cnn+max+drop.h5\n",
      "61597/61597 [==============================] - 344s 6ms/step - loss: 1.0812 - acc: 0.7680 - val_loss: 1.2081 - val_acc: 0.7163\n",
      "Epoch 7/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0701 - acc: 0.7688Epoch 00007: val_loss did not improve\n",
      "61597/61597 [==============================] - 343s 6ms/step - loss: 1.0701 - acc: 0.7689 - val_loss: 1.2237 - val_acc: 0.7541\n",
      "Epoch 8/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0609 - acc: 0.7677Epoch 00008: val_loss did not improve\n",
      "61597/61597 [==============================] - 340s 6ms/step - loss: 1.0608 - acc: 0.7677 - val_loss: 1.2360 - val_acc: 0.7867\n",
      "Epoch 9/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0530 - acc: 0.7690Epoch 00009: val_loss did not improve\n",
      "61597/61597 [==============================] - 340s 6ms/step - loss: 1.0530 - acc: 0.7689 - val_loss: 1.2463 - val_acc: 0.6849\n",
      "Epoch 10/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0471 - acc: 0.7679Epoch 00010: val_loss did not improve\n",
      "61597/61597 [==============================] - 339s 6ms/step - loss: 1.0469 - acc: 0.7680 - val_loss: 1.2624 - val_acc: 0.7057\n",
      "Epoch 11/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0403 - acc: 0.7696Epoch 00011: val_loss did not improve\n",
      "61597/61597 [==============================] - 339s 6ms/step - loss: 1.0403 - acc: 0.7696 - val_loss: 1.2729 - val_acc: 0.7218\n",
      "Epoch 12/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0355 - acc: 0.7692Epoch 00012: val_loss did not improve\n",
      "61597/61597 [==============================] - 339s 6ms/step - loss: 1.0355 - acc: 0.7693 - val_loss: 1.2996 - val_acc: 0.7481\n",
      "Epoch 13/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0320 - acc: 0.7676Epoch 00013: val_loss did not improve\n",
      "61597/61597 [==============================] - 340s 6ms/step - loss: 1.0319 - acc: 0.7676 - val_loss: 1.3133 - val_acc: 0.7840\n",
      "Epoch 14/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0276 - acc: 0.7705Epoch 00014: val_loss did not improve\n",
      "61597/61597 [==============================] - 339s 6ms/step - loss: 1.0276 - acc: 0.7706 - val_loss: 1.3452 - val_acc: 0.8456\n",
      "Epoch 15/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0254 - acc: 0.7694Epoch 00015: val_loss did not improve\n",
      "61597/61597 [==============================] - 340s 6ms/step - loss: 1.0254 - acc: 0.7694 - val_loss: 1.3556 - val_acc: 0.7400\n",
      "Epoch 16/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0219 - acc: 0.7667Epoch 00016: val_loss did not improve\n",
      "61597/61597 [==============================] - 339s 6ms/step - loss: 1.0219 - acc: 0.7667 - val_loss: 1.3570 - val_acc: 0.8081\n",
      "Wall time: 1h 31min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# model.optimizer.lr=1e-4\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "# early_stopping = EarlyStopping(monitor='val_acc', patience=10)\n",
    "\n",
    "sub = \"2cnn+max+drop\"\n",
    "model_chk_path = 'model/cnn_hashtagcls_emb{}_weights{}.h5'.format(embedding_dim, sub)\n",
    "mcp = ModelCheckpoint(model_chk_path, monitor=\"val_loss\", verbose=1,\n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "model.fit(X_train_u, y_train, validation_data=(X_test_u, y_test), epochs=500, batch_size=64, callbacks=[early_stopping, mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 39, 200)           4661400   \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 39, 200)           200200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 39, 200)           800       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 39, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 39, 200)           200200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 39, 200)           800       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 39, 200)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 7800)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 11)                85811     \n",
      "=================================================================\n",
      "Total params: 5,149,211\n",
      "Trainable params: 5,148,411\n",
      "Non-trainable params: 800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#pure cnn model\n",
    "dth = 0.5\n",
    "filter_num = 5\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#embedding layer\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=seq_len, weights=[embedding], trainable=True))\n",
    "# model.add(Dropout(dth))\n",
    "\n",
    "#hidden layer\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "# model.add(Conv1D(embedding_dim, filter_num, padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Conv1D(embedding_dim, filter_num, padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Conv1D(embedding_dim, filter_num, padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Conv1D(embedding_dim, filter_num, padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "#output layer\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(dth))\n",
    "model.add(Dense(modeling_data['categorical_num'], activation='softmax'))\n",
    "\n",
    "#compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61597 samples, validate on 16210 samples\n",
      "Epoch 1/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.5776 - acc: 0.7227Epoch 00001: val_loss improved from inf to 1.38550, saving model to model/cnn_hashtagcls_emb200_weights2cnn_bn_ac.h5\n",
      "61597/61597 [==============================] - 390s 6ms/step - loss: 1.5777 - acc: 0.7228 - val_loss: 1.3855 - val_acc: 0.7319\n",
      "Epoch 2/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.2564 - acc: 0.7597Epoch 00002: val_loss improved from 1.38550 to 1.28829, saving model to model/cnn_hashtagcls_emb200_weights2cnn_bn_ac.h5\n",
      "61597/61597 [==============================] - 381s 6ms/step - loss: 1.2565 - acc: 0.7597 - val_loss: 1.2883 - val_acc: 0.6893\n",
      "Epoch 3/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1614 - acc: 0.7665Epoch 00003: val_loss improved from 1.28829 to 1.23510, saving model to model/cnn_hashtagcls_emb200_weights2cnn_bn_ac.h5\n",
      "61597/61597 [==============================] - 384s 6ms/step - loss: 1.1616 - acc: 0.7664 - val_loss: 1.2351 - val_acc: 0.7563\n",
      "Epoch 4/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1194 - acc: 0.7689Epoch 00004: val_loss did not improve\n",
      "61597/61597 [==============================] - 386s 6ms/step - loss: 1.1191 - acc: 0.7689 - val_loss: 1.2407 - val_acc: 0.7532\n",
      "Epoch 5/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0921 - acc: 0.7687Epoch 00005: val_loss improved from 1.23510 to 1.23438, saving model to model/cnn_hashtagcls_emb200_weights2cnn_bn_ac.h5\n",
      "61597/61597 [==============================] - 384s 6ms/step - loss: 1.0920 - acc: 0.7687 - val_loss: 1.2344 - val_acc: 0.7721\n",
      "Epoch 6/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0747 - acc: 0.7705Epoch 00006: val_loss improved from 1.23438 to 1.21345, saving model to model/cnn_hashtagcls_emb200_weights2cnn_bn_ac.h5\n",
      "61597/61597 [==============================] - 384s 6ms/step - loss: 1.0748 - acc: 0.7705 - val_loss: 1.2135 - val_acc: 0.7458\n",
      "Epoch 7/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0626 - acc: 0.7699Epoch 00007: val_loss did not improve\n",
      "61597/61597 [==============================] - 384s 6ms/step - loss: 1.0626 - acc: 0.7700 - val_loss: 1.2230 - val_acc: 0.7966\n",
      "Epoch 8/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0536 - acc: 0.7708Epoch 00008: val_loss improved from 1.21345 to 1.20984, saving model to model/cnn_hashtagcls_emb200_weights2cnn_bn_ac.h5\n",
      "61597/61597 [==============================] - 384s 6ms/step - loss: 1.0536 - acc: 0.7708 - val_loss: 1.2098 - val_acc: 0.6689\n",
      "Epoch 9/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0414 - acc: 0.7702Epoch 00009: val_loss improved from 1.20984 to 1.20585, saving model to model/cnn_hashtagcls_emb200_weights2cnn_bn_ac.h5\n",
      "61597/61597 [==============================] - 383s 6ms/step - loss: 1.0414 - acc: 0.7702 - val_loss: 1.2058 - val_acc: 0.7518\n",
      "Epoch 10/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0355 - acc: 0.7706Epoch 00010: val_loss improved from 1.20585 to 1.20282, saving model to model/cnn_hashtagcls_emb200_weights2cnn_bn_ac.h5\n",
      "61597/61597 [==============================] - 384s 6ms/step - loss: 1.0352 - acc: 0.7707 - val_loss: 1.2028 - val_acc: 0.7072\n",
      "Epoch 11/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0286 - acc: 0.7702Epoch 00011: val_loss did not improve\n",
      "61597/61597 [==============================] - 382s 6ms/step - loss: 1.0286 - acc: 0.7702 - val_loss: 1.2041 - val_acc: 0.7113\n",
      "Epoch 12/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0227 - acc: 0.7694Epoch 00012: val_loss did not improve\n",
      "61597/61597 [==============================] - 383s 6ms/step - loss: 1.0228 - acc: 0.7693 - val_loss: 1.2167 - val_acc: 0.7258\n",
      "Epoch 13/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0188 - acc: 0.7707Epoch 00013: val_loss did not improve\n",
      "61597/61597 [==============================] - 383s 6ms/step - loss: 1.0186 - acc: 0.7707 - val_loss: 1.2120 - val_acc: 0.7608\n",
      "Epoch 14/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0140 - acc: 0.7707Epoch 00014: val_loss did not improve\n",
      "61597/61597 [==============================] - 382s 6ms/step - loss: 1.0142 - acc: 0.7706 - val_loss: 1.2154 - val_acc: 0.7002\n",
      "Epoch 15/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0107 - acc: 0.7698Epoch 00015: val_loss did not improve\n",
      "61597/61597 [==============================] - 381s 6ms/step - loss: 1.0109 - acc: 0.7698 - val_loss: 1.2281 - val_acc: 0.7628\n",
      "Epoch 16/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0078 - acc: 0.7701Epoch 00016: val_loss did not improve\n",
      "61597/61597 [==============================] - 383s 6ms/step - loss: 1.0077 - acc: 0.7701 - val_loss: 1.2216 - val_acc: 0.7152\n",
      "Epoch 17/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0044 - acc: 0.7721Epoch 00017: val_loss did not improve\n",
      "61597/61597 [==============================] - 387s 6ms/step - loss: 1.0044 - acc: 0.7721 - val_loss: 1.2320 - val_acc: 0.6943\n",
      "Epoch 18/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0018 - acc: 0.7691Epoch 00018: val_loss did not improve\n",
      "61597/61597 [==============================] - 388s 6ms/step - loss: 1.0017 - acc: 0.7691 - val_loss: 1.2452 - val_acc: 0.7138\n",
      "Epoch 19/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.9984 - acc: 0.7696Epoch 00019: val_loss did not improve\n",
      "61597/61597 [==============================] - 394s 6ms/step - loss: 0.9984 - acc: 0.7696 - val_loss: 1.2389 - val_acc: 0.7779\n",
      "Epoch 20/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.9982 - acc: 0.7712Epoch 00020: val_loss did not improve\n",
      "61597/61597 [==============================] - 383s 6ms/step - loss: 0.9981 - acc: 0.7711 - val_loss: 1.2393 - val_acc: 0.7600\n",
      "Wall time: 2h 8min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "# early_stopping = EarlyStopping(monitor='val_acc', patience=10)\n",
    "\n",
    "# sub = \"used_for_prediction_bn\"\n",
    "sub = \"2cnn_bn_ac\"\n",
    "model_chk_path = 'model/cnn_hashtagcls_emb{}_weights{}.h5'.format(embedding_dim, sub)\n",
    "mcp = ModelCheckpoint(model_chk_path, monitor=\"val_loss\", verbose=1,\n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "model.fit(X_train_u, y_train, validation_data=(X_test_u, y_test), epochs=500, batch_size=64, callbacks=[early_stopping, mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 39, 200)           4661400   \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 39, 200)           200200    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 39, 200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 400)               641600    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 11)                4411      \n",
      "=================================================================\n",
      "Total params: 5,507,611\n",
      "Trainable params: 846,211\n",
      "Non-trainable params: 4,661,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#cnn Bi-LSTM model\n",
    "dth = 0.5\n",
    "filter_num = 5\n",
    "model = Sequential()\n",
    "\n",
    "#embedding layer\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=seq_len, weights=[embedding], trainable=False))\n",
    "# model.add(Dropout(dth))\n",
    "\n",
    "#hidden layers\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Dropout(dth))\n",
    "model.add(Bidirectional(LSTM(embedding_dim, dropout=0.5, recurrent_dropout=0.5)))\n",
    "# model.add(Bidirectional(LSTM(embedding_dim,  batch_input_shape=(64, 39, 50), stateful=True)))\n",
    "# model.add(Dropout(dth))\n",
    "\n",
    "#output layer\n",
    "model.add(Dense(modeling_data['categorical_num'], activation='softmax'))\n",
    "\n",
    "#compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61597 samples, validate on 16210 samples\n",
      "Epoch 1/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.6368 - acc: 0.7299Epoch 00001: val_loss improved from inf to 1.30201, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1109s 18ms/step - loss: 1.6367 - acc: 0.7299 - val_loss: 1.3020 - val_acc: 0.8057\n",
      "Epoch 2/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.3164 - acc: 0.7596Epoch 00002: val_loss improved from 1.30201 to 1.23744, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1091s 18ms/step - loss: 1.3162 - acc: 0.7597 - val_loss: 1.2374 - val_acc: 0.7546\n",
      "Epoch 3/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.2596 - acc: 0.7613Epoch 00003: val_loss improved from 1.23744 to 1.21709, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1086s 18ms/step - loss: 1.2596 - acc: 0.7613 - val_loss: 1.2171 - val_acc: 0.8152\n",
      "Epoch 4/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.2327 - acc: 0.7620Epoch 00004: val_loss improved from 1.21709 to 1.19551, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1081s 18ms/step - loss: 1.2328 - acc: 0.7620 - val_loss: 1.1955 - val_acc: 0.7759\n",
      "Epoch 5/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.2123 - acc: 0.7631Epoch 00005: val_loss improved from 1.19551 to 1.19292, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1084s 18ms/step - loss: 1.2121 - acc: 0.7631 - val_loss: 1.1929 - val_acc: 0.6832\n",
      "Epoch 6/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.2012 - acc: 0.7620Epoch 00006: val_loss improved from 1.19292 to 1.17559, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1078s 17ms/step - loss: 1.2011 - acc: 0.7620 - val_loss: 1.1756 - val_acc: 0.7267\n",
      "Epoch 7/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1886 - acc: 0.7614Epoch 00007: val_loss improved from 1.17559 to 1.17478, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1080s 18ms/step - loss: 1.1884 - acc: 0.7614 - val_loss: 1.1748 - val_acc: 0.8436\n",
      "Epoch 8/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1776 - acc: 0.7621Epoch 00008: val_loss improved from 1.17478 to 1.16632, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1083s 18ms/step - loss: 1.1776 - acc: 0.7621 - val_loss: 1.1663 - val_acc: 0.7672\n",
      "Epoch 9/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1699 - acc: 0.7630Epoch 00009: val_loss improved from 1.16632 to 1.15751, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1092s 18ms/step - loss: 1.1699 - acc: 0.7629 - val_loss: 1.1575 - val_acc: 0.7519\n",
      "Epoch 10/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1646 - acc: 0.7627Epoch 00010: val_loss improved from 1.15751 to 1.15426, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1083s 18ms/step - loss: 1.1647 - acc: 0.7627 - val_loss: 1.1543 - val_acc: 0.7647\n",
      "Epoch 11/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1576 - acc: 0.7631Epoch 00011: val_loss did not improve\n",
      "61597/61597 [==============================] - 1082s 18ms/step - loss: 1.1577 - acc: 0.7630 - val_loss: 1.1549 - val_acc: 0.7510\n",
      "Epoch 12/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1511 - acc: 0.7618Epoch 00012: val_loss improved from 1.15426 to 1.15180, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1084s 18ms/step - loss: 1.1512 - acc: 0.7619 - val_loss: 1.1518 - val_acc: 0.7981\n",
      "Epoch 13/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1464 - acc: 0.7616Epoch 00013: val_loss improved from 1.15180 to 1.15053, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1089s 18ms/step - loss: 1.1462 - acc: 0.7617 - val_loss: 1.1505 - val_acc: 0.8385\n",
      "Epoch 14/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1395 - acc: 0.7607Epoch 00014: val_loss improved from 1.15053 to 1.14359, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1086s 18ms/step - loss: 1.1399 - acc: 0.7607 - val_loss: 1.1436 - val_acc: 0.7384\n",
      "Epoch 15/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1360 - acc: 0.7607Epoch 00015: val_loss improved from 1.14359 to 1.13982, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1088s 18ms/step - loss: 1.1360 - acc: 0.7607 - val_loss: 1.1398 - val_acc: 0.7731\n",
      "Epoch 16/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1310 - acc: 0.7601Epoch 00016: val_loss improved from 1.13982 to 1.13852, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1081s 18ms/step - loss: 1.1307 - acc: 0.7602 - val_loss: 1.1385 - val_acc: 0.8108\n",
      "Epoch 17/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1295 - acc: 0.7608Epoch 00017: val_loss did not improve\n",
      "61597/61597 [==============================] - 1089s 18ms/step - loss: 1.1294 - acc: 0.7607 - val_loss: 1.1409 - val_acc: 0.7481\n",
      "Epoch 18/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1261 - acc: 0.7566Epoch 00018: val_loss improved from 1.13852 to 1.13757, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1092s 18ms/step - loss: 1.1262 - acc: 0.7566 - val_loss: 1.1376 - val_acc: 0.8191\n",
      "Epoch 19/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1227 - acc: 0.7595Epoch 00019: val_loss did not improve\n",
      "61597/61597 [==============================] - 1175s 19ms/step - loss: 1.1225 - acc: 0.7595 - val_loss: 1.1382 - val_acc: 0.7198\n",
      "Epoch 20/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1187 - acc: 0.7589Epoch 00020: val_loss did not improve\n",
      "61597/61597 [==============================] - 1090s 18ms/step - loss: 1.1189 - acc: 0.7588 - val_loss: 1.1380 - val_acc: 0.7536\n",
      "Epoch 21/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1195 - acc: 0.7575Epoch 00021: val_loss improved from 1.13757 to 1.13623, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1080s 18ms/step - loss: 1.1194 - acc: 0.7575 - val_loss: 1.1362 - val_acc: 0.7553\n",
      "Epoch 22/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1138 - acc: 0.7596Epoch 00022: val_loss did not improve\n",
      "61597/61597 [==============================] - 1069s 17ms/step - loss: 1.1139 - acc: 0.7595 - val_loss: 1.1377 - val_acc: 0.7711\n",
      "Epoch 23/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1127 - acc: 0.7600Epoch 00023: val_loss improved from 1.13623 to 1.13263, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1075s 17ms/step - loss: 1.1127 - acc: 0.7599 - val_loss: 1.1326 - val_acc: 0.7382\n",
      "Epoch 24/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1093 - acc: 0.7573Epoch 00024: val_loss did not improve\n",
      "61597/61597 [==============================] - 1072s 17ms/step - loss: 1.1093 - acc: 0.7574 - val_loss: 1.1355 - val_acc: 0.7700\n",
      "Epoch 25/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1076 - acc: 0.7568Epoch 00025: val_loss did not improve\n",
      "61597/61597 [==============================] - 1077s 17ms/step - loss: 1.1078 - acc: 0.7568 - val_loss: 1.1350 - val_acc: 0.7040\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1059 - acc: 0.7588Epoch 00026: val_loss did not improve\n",
      "61597/61597 [==============================] - 1083s 18ms/step - loss: 1.1061 - acc: 0.7587 - val_loss: 1.1339 - val_acc: 0.7672\n",
      "Epoch 27/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1039 - acc: 0.7570Epoch 00027: val_loss did not improve\n",
      "61597/61597 [==============================] - 1113s 18ms/step - loss: 1.1040 - acc: 0.7569 - val_loss: 1.1345 - val_acc: 0.8139\n",
      "Epoch 28/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1031 - acc: 0.7573Epoch 00028: val_loss did not improve\n",
      "61597/61597 [==============================] - 1111s 18ms/step - loss: 1.1032 - acc: 0.7572 - val_loss: 1.1329 - val_acc: 0.6941\n",
      "Epoch 29/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0995 - acc: 0.7557Epoch 00029: val_loss did not improve\n",
      "61597/61597 [==============================] - 1112s 18ms/step - loss: 1.0997 - acc: 0.7557 - val_loss: 1.1390 - val_acc: 0.7692\n",
      "Epoch 30/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0989 - acc: 0.7564Epoch 00030: val_loss did not improve\n",
      "61597/61597 [==============================] - 1111s 18ms/step - loss: 1.0989 - acc: 0.7564 - val_loss: 1.1332 - val_acc: 0.7661\n",
      "Epoch 31/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0983 - acc: 0.7549Epoch 00031: val_loss improved from 1.13263 to 1.13220, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1111s 18ms/step - loss: 1.0983 - acc: 0.7549 - val_loss: 1.1322 - val_acc: 0.7145\n",
      "Epoch 32/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0970 - acc: 0.7565Epoch 00032: val_loss did not improve\n",
      "61597/61597 [==============================] - 1113s 18ms/step - loss: 1.0970 - acc: 0.7566 - val_loss: 1.1360 - val_acc: 0.7603\n",
      "Epoch 33/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0953 - acc: 0.7539Epoch 00033: val_loss did not improve\n",
      "61597/61597 [==============================] - 1165s 19ms/step - loss: 1.0953 - acc: 0.7539 - val_loss: 1.1359 - val_acc: 0.7752\n",
      "Epoch 34/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0959 - acc: 0.7528Epoch 00034: val_loss did not improve\n",
      "61597/61597 [==============================] - 1920s 31ms/step - loss: 1.0957 - acc: 0.7528 - val_loss: 1.1335 - val_acc: 0.7660\n",
      "Epoch 35/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0925 - acc: 0.7541Epoch 00035: val_loss did not improve\n",
      "61597/61597 [==============================] - 1244s 20ms/step - loss: 1.0925 - acc: 0.7541 - val_loss: 1.1331 - val_acc: 0.7174\n",
      "Epoch 36/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0915 - acc: 0.7573Epoch 00036: val_loss improved from 1.13220 to 1.13162, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1104s 18ms/step - loss: 1.0916 - acc: 0.7573 - val_loss: 1.1316 - val_acc: 0.7375\n",
      "Epoch 37/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0909 - acc: 0.7519Epoch 00037: val_loss did not improve\n",
      "61597/61597 [==============================] - 1103s 18ms/step - loss: 1.0910 - acc: 0.7520 - val_loss: 1.1390 - val_acc: 0.7782\n",
      "Epoch 38/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0891 - acc: 0.7547Epoch 00038: val_loss did not improve\n",
      "61597/61597 [==============================] - 1093s 18ms/step - loss: 1.0891 - acc: 0.7547 - val_loss: 1.1388 - val_acc: 0.7714\n",
      "Epoch 39/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0871 - acc: 0.7549Epoch 00039: val_loss did not improve\n",
      "61597/61597 [==============================] - 1096s 18ms/step - loss: 1.0873 - acc: 0.7548 - val_loss: 1.1331 - val_acc: 0.7151\n",
      "Epoch 40/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0877 - acc: 0.7544Epoch 00040: val_loss did not improve\n",
      "61597/61597 [==============================] - 1101s 18ms/step - loss: 1.0875 - acc: 0.7544 - val_loss: 1.1378 - val_acc: 0.7849\n",
      "Epoch 41/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0854 - acc: 0.7540Epoch 00041: val_loss improved from 1.13162 to 1.13052, saving model to model/cnn_hashtagcls_emb200_weightscnn_biLSTM.h5\n",
      "61597/61597 [==============================] - 1100s 18ms/step - loss: 1.0857 - acc: 0.7540 - val_loss: 1.1305 - val_acc: 0.8061\n",
      "Epoch 42/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0858 - acc: 0.7536Epoch 00042: val_loss did not improve\n",
      "61597/61597 [==============================] - 1099s 18ms/step - loss: 1.0861 - acc: 0.7536 - val_loss: 1.1418 - val_acc: 0.7244\n",
      "Epoch 43/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0851 - acc: 0.7508Epoch 00043: val_loss did not improve\n",
      "61597/61597 [==============================] - 1101s 18ms/step - loss: 1.0851 - acc: 0.7508 - val_loss: 1.1378 - val_acc: 0.8170\n",
      "Epoch 44/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0850 - acc: 0.7544Epoch 00044: val_loss did not improve\n",
      "61597/61597 [==============================] - 1102s 18ms/step - loss: 1.0849 - acc: 0.7544 - val_loss: 1.1428 - val_acc: 0.7289\n",
      "Epoch 45/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0836 - acc: 0.7530Epoch 00045: val_loss did not improve\n",
      "61597/61597 [==============================] - 1102s 18ms/step - loss: 1.0837 - acc: 0.7529 - val_loss: 1.1391 - val_acc: 0.7444\n",
      "Epoch 46/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0824 - acc: 0.7536Epoch 00046: val_loss did not improve\n",
      "61597/61597 [==============================] - 1096s 18ms/step - loss: 1.0825 - acc: 0.7536 - val_loss: 1.1350 - val_acc: 0.8002\n",
      "Epoch 47/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0791 - acc: 0.7519Epoch 00047: val_loss did not improve\n",
      "61597/61597 [==============================] - 1221s 20ms/step - loss: 1.0791 - acc: 0.7519 - val_loss: 1.1448 - val_acc: 0.7656\n",
      "Epoch 48/500\n",
      "34176/61597 [===============>..............] - ETA: 11:05 - loss: 1.0872 - acc: 0.7506"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-0ee7bcc1b4c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m mcp = ModelCheckpoint(model_chk_path, monitor=\"val_loss\", verbose=1,\n\u001b[0;32m      8\u001b[0m                       save_best_only=True, save_weights_only=False)\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_u\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_u\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmcp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\conda\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mE:\\conda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mE:\\conda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\conda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\conda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\conda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\conda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\conda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\conda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model.optimizer.lr=1e-4\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "# early_stopping = EarlyStopping(monitor='val_acc', patience=10)\n",
    "\n",
    "sub = \"cnn_biLSTM\"\n",
    "model_chk_path = 'model/cnn_hashtagcls_emb{}_weights{}.h5'.format(embedding_dim, sub)\n",
    "mcp = ModelCheckpoint(model_chk_path, monitor=\"val_loss\", verbose=1,\n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "model.fit(X_train_u, y_train, validation_data=(X_test_u, y_test), epochs=500, batch_size=64, callbacks=[early_stopping, mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 39, 200)           4661400   \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 400)               641600    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 11)                4411      \n",
      "=================================================================\n",
      "Total params: 5,307,411\n",
      "Trainable params: 646,011\n",
      "Non-trainable params: 4,661,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Bi-LSTM model\n",
    "dth = 0.5\n",
    "filter_num = 5\n",
    "model = Sequential()\n",
    "\n",
    "#embedding layer\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=seq_len, weights=[embedding], trainable=False))\n",
    "# model.add(Dropout(dth))\n",
    "\n",
    "#hidden layers\n",
    "# model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "# model.add(Dropout(dth))\n",
    "model.add(Bidirectional(LSTM(embedding_dim, dropout=0.5, recurrent_dropout=0.5)))\n",
    "# model.add(Bidirectional(LSTM(embedding_dim,  batch_input_shape=(64, 39, 50), stateful=True)))\n",
    "# model.add(Dropout(dth))\n",
    "\n",
    "#output layer\n",
    "model.add(Dense(modeling_data['categorical_num'], activation='softmax'))\n",
    "\n",
    "#compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61597 samples, validate on 16210 samples\n",
      "Epoch 1/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.2647 - acc: 0.7653Epoch 00001: val_loss improved from inf to 1.21642, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 1145s 19ms/step - loss: 1.2645 - acc: 0.7654 - val_loss: 1.2164 - val_acc: 0.7463\n",
      "Epoch 2/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.2444 - acc: 0.7668Epoch 00002: val_loss improved from 1.21642 to 1.20550, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 1027s 17ms/step - loss: 1.2444 - acc: 0.7668 - val_loss: 1.2055 - val_acc: 0.7378\n",
      "Epoch 3/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.2320 - acc: 0.7671Epoch 00003: val_loss improved from 1.20550 to 1.20399, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 871s 14ms/step - loss: 1.2319 - acc: 0.7671 - val_loss: 1.2040 - val_acc: 0.7174\n",
      "Epoch 4/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.2193 - acc: 0.7659Epoch 00004: val_loss improved from 1.20399 to 1.19007, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 871s 14ms/step - loss: 1.2192 - acc: 0.7658 - val_loss: 1.1901 - val_acc: 0.7734\n",
      "Epoch 5/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.2091 - acc: 0.7698Epoch 00005: val_loss improved from 1.19007 to 1.18085, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 870s 14ms/step - loss: 1.2092 - acc: 0.7698 - val_loss: 1.1809 - val_acc: 0.7236\n",
      "Epoch 6/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.2014 - acc: 0.7682Epoch 00006: val_loss improved from 1.18085 to 1.17497, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 873s 14ms/step - loss: 1.2015 - acc: 0.7681 - val_loss: 1.1750 - val_acc: 0.7559\n",
      "Epoch 7/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1942 - acc: 0.7687Epoch 00007: val_loss improved from 1.17497 to 1.17353, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 871s 14ms/step - loss: 1.1945 - acc: 0.7686 - val_loss: 1.1735 - val_acc: 0.7717\n",
      "Epoch 8/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1889 - acc: 0.7700Epoch 00008: val_loss improved from 1.17353 to 1.16875, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 875s 14ms/step - loss: 1.1889 - acc: 0.7700 - val_loss: 1.1688 - val_acc: 0.7231\n",
      "Epoch 9/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1827 - acc: 0.7682Epoch 00009: val_loss improved from 1.16875 to 1.16360, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 864s 14ms/step - loss: 1.1828 - acc: 0.7681 - val_loss: 1.1636 - val_acc: 0.7306\n",
      "Epoch 10/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1796 - acc: 0.7684Epoch 00010: val_loss improved from 1.16360 to 1.16002, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 868s 14ms/step - loss: 1.1797 - acc: 0.7684 - val_loss: 1.1600 - val_acc: 0.8085\n",
      "Epoch 11/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1735 - acc: 0.7689Epoch 00011: val_loss improved from 1.16002 to 1.15809, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 876s 14ms/step - loss: 1.1736 - acc: 0.7689 - val_loss: 1.1581 - val_acc: 0.7247\n",
      "Epoch 12/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1699 - acc: 0.7696Epoch 00012: val_loss improved from 1.15809 to 1.15476, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 879s 14ms/step - loss: 1.1699 - acc: 0.7697 - val_loss: 1.1548 - val_acc: 0.7437\n",
      "Epoch 13/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1653 - acc: 0.7679Epoch 00013: val_loss improved from 1.15476 to 1.15034, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 872s 14ms/step - loss: 1.1653 - acc: 0.7679 - val_loss: 1.1503 - val_acc: 0.7403\n",
      "Epoch 14/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1633 - acc: 0.7699Epoch 00014: val_loss improved from 1.15034 to 1.14907, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 874s 14ms/step - loss: 1.1633 - acc: 0.7699 - val_loss: 1.1491 - val_acc: 0.7571\n",
      "Epoch 15/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1595 - acc: 0.7697Epoch 00015: val_loss did not improve\n",
      "61597/61597 [==============================] - 868s 14ms/step - loss: 1.1596 - acc: 0.7697 - val_loss: 1.1505 - val_acc: 0.7309\n",
      "Epoch 16/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1576 - acc: 0.7700Epoch 00016: val_loss improved from 1.14907 to 1.14586, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 878s 14ms/step - loss: 1.1575 - acc: 0.7700 - val_loss: 1.1459 - val_acc: 0.6967\n",
      "Epoch 17/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1542 - acc: 0.7700Epoch 00017: val_loss improved from 1.14586 to 1.14168, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 866s 14ms/step - loss: 1.1543 - acc: 0.7700 - val_loss: 1.1417 - val_acc: 0.7324\n",
      "Epoch 18/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1520 - acc: 0.7700Epoch 00018: val_loss did not improve\n",
      "61597/61597 [==============================] - 872s 14ms/step - loss: 1.1521 - acc: 0.7699 - val_loss: 1.1424 - val_acc: 0.7397\n",
      "Epoch 19/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1486 - acc: 0.7707Epoch 00019: val_loss did not improve\n",
      "61597/61597 [==============================] - 872s 14ms/step - loss: 1.1487 - acc: 0.7708 - val_loss: 1.1419 - val_acc: 0.7365\n",
      "Epoch 20/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1464 - acc: 0.7686Epoch 00020: val_loss improved from 1.14168 to 1.13654, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 875s 14ms/step - loss: 1.1466 - acc: 0.7685 - val_loss: 1.1365 - val_acc: 0.7944\n",
      "Epoch 21/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1449 - acc: 0.7715Epoch 00021: val_loss did not improve\n",
      "61597/61597 [==============================] - 876s 14ms/step - loss: 1.1447 - acc: 0.7715 - val_loss: 1.1365 - val_acc: 0.7532\n",
      "Epoch 22/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1425 - acc: 0.7689Epoch 00022: val_loss improved from 1.13654 to 1.13616, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 878s 14ms/step - loss: 1.1425 - acc: 0.7689 - val_loss: 1.1362 - val_acc: 0.7127\n",
      "Epoch 23/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1395 - acc: 0.7693Epoch 00023: val_loss improved from 1.13616 to 1.13349, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 871s 14ms/step - loss: 1.1397 - acc: 0.7693 - val_loss: 1.1335 - val_acc: 0.7553\n",
      "Epoch 24/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1388 - acc: 0.7709Epoch 00024: val_loss did not improve\n",
      "61597/61597 [==============================] - 868s 14ms/step - loss: 1.1388 - acc: 0.7708 - val_loss: 1.1370 - val_acc: 0.7116\n",
      "Epoch 25/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1371 - acc: 0.7707Epoch 00025: val_loss did not improve\n",
      "61597/61597 [==============================] - 873s 14ms/step - loss: 1.1372 - acc: 0.7706 - val_loss: 1.1383 - val_acc: 0.7229\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1348 - acc: 0.7700Epoch 00026: val_loss improved from 1.13349 to 1.13005, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 867s 14ms/step - loss: 1.1349 - acc: 0.7701 - val_loss: 1.1301 - val_acc: 0.7598\n",
      "Epoch 27/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1347 - acc: 0.7727Epoch 00027: val_loss improved from 1.13005 to 1.12811, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 867s 14ms/step - loss: 1.1348 - acc: 0.7727 - val_loss: 1.1281 - val_acc: 0.7533\n",
      "Epoch 28/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1322 - acc: 0.7708Epoch 00028: val_loss did not improve\n",
      "61597/61597 [==============================] - 865s 14ms/step - loss: 1.1322 - acc: 0.7708 - val_loss: 1.1318 - val_acc: 0.7271\n",
      "Epoch 29/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1314 - acc: 0.7697Epoch 00029: val_loss did not improve\n",
      "61597/61597 [==============================] - 866s 14ms/step - loss: 1.1313 - acc: 0.7697 - val_loss: 1.1289 - val_acc: 0.7350\n",
      "Epoch 30/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1298 - acc: 0.7697Epoch 00030: val_loss improved from 1.12811 to 1.12775, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 861s 14ms/step - loss: 1.1297 - acc: 0.7698 - val_loss: 1.1278 - val_acc: 0.7413\n",
      "Epoch 31/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1300 - acc: 0.7705Epoch 00031: val_loss improved from 1.12775 to 1.12737, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 870s 14ms/step - loss: 1.1300 - acc: 0.7705 - val_loss: 1.1274 - val_acc: 0.7017\n",
      "Epoch 32/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1269 - acc: 0.7690Epoch 00032: val_loss improved from 1.12737 to 1.12694, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 896s 15ms/step - loss: 1.1270 - acc: 0.7690 - val_loss: 1.1269 - val_acc: 0.7382\n",
      "Epoch 33/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1270 - acc: 0.7710Epoch 00033: val_loss improved from 1.12694 to 1.12473, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 888s 14ms/step - loss: 1.1271 - acc: 0.7711 - val_loss: 1.1247 - val_acc: 0.8004\n",
      "Epoch 34/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1257 - acc: 0.7685Epoch 00034: val_loss improved from 1.12473 to 1.12358, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 885s 14ms/step - loss: 1.1257 - acc: 0.7685 - val_loss: 1.1236 - val_acc: 0.7334\n",
      "Epoch 35/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1233 - acc: 0.7676Epoch 00035: val_loss did not improve\n",
      "61597/61597 [==============================] - 877s 14ms/step - loss: 1.1233 - acc: 0.7676 - val_loss: 1.1238 - val_acc: 0.7328\n",
      "Epoch 36/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1230 - acc: 0.7681Epoch 00036: val_loss improved from 1.12358 to 1.12342, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 883s 14ms/step - loss: 1.1230 - acc: 0.7680 - val_loss: 1.1234 - val_acc: 0.7595\n",
      "Epoch 37/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1220 - acc: 0.7702Epoch 00037: val_loss improved from 1.12342 to 1.12259, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 874s 14ms/step - loss: 1.1221 - acc: 0.7702 - val_loss: 1.1226 - val_acc: 0.7557\n",
      "Epoch 38/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1212 - acc: 0.7708Epoch 00038: val_loss did not improve\n",
      "61597/61597 [==============================] - 871s 14ms/step - loss: 1.1209 - acc: 0.7708 - val_loss: 1.1229 - val_acc: 0.7155\n",
      "Epoch 39/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1208 - acc: 0.7685Epoch 00039: val_loss improved from 1.12259 to 1.12241, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 870s 14ms/step - loss: 1.1209 - acc: 0.7685 - val_loss: 1.1224 - val_acc: 0.7844\n",
      "Epoch 40/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1197 - acc: 0.7692Epoch 00040: val_loss improved from 1.12241 to 1.12171, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 873s 14ms/step - loss: 1.1197 - acc: 0.7692 - val_loss: 1.1217 - val_acc: 0.7204\n",
      "Epoch 41/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1185 - acc: 0.7704Epoch 00041: val_loss improved from 1.12171 to 1.12170, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 880s 14ms/step - loss: 1.1185 - acc: 0.7704 - val_loss: 1.1217 - val_acc: 0.7078\n",
      "Epoch 42/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1175 - acc: 0.7693Epoch 00042: val_loss improved from 1.12170 to 1.12071, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 879s 14ms/step - loss: 1.1174 - acc: 0.7694 - val_loss: 1.1207 - val_acc: 0.7842\n",
      "Epoch 43/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1163 - acc: 0.7687Epoch 00043: val_loss did not improve\n",
      "61597/61597 [==============================] - 889s 14ms/step - loss: 1.1164 - acc: 0.7687 - val_loss: 1.1221 - val_acc: 0.7423\n",
      "Epoch 44/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1162 - acc: 0.7707Epoch 00044: val_loss improved from 1.12071 to 1.11864, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 876s 14ms/step - loss: 1.1162 - acc: 0.7708 - val_loss: 1.1186 - val_acc: 0.7941\n",
      "Epoch 45/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1147 - acc: 0.7688Epoch 00045: val_loss did not improve\n",
      "61597/61597 [==============================] - 882s 14ms/step - loss: 1.1146 - acc: 0.7688 - val_loss: 1.1199 - val_acc: 0.7866\n",
      "Epoch 46/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1155 - acc: 0.7698Epoch 00046: val_loss improved from 1.11864 to 1.11759, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 881s 14ms/step - loss: 1.1156 - acc: 0.7698 - val_loss: 1.1176 - val_acc: 0.7589\n",
      "Epoch 47/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1134 - acc: 0.7686Epoch 00047: val_loss did not improve\n",
      "61597/61597 [==============================] - 882s 14ms/step - loss: 1.1135 - acc: 0.7685 - val_loss: 1.1236 - val_acc: 0.7147\n",
      "Epoch 48/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1126 - acc: 0.7678Epoch 00048: val_loss did not improve\n",
      "61597/61597 [==============================] - 882s 14ms/step - loss: 1.1125 - acc: 0.7678 - val_loss: 1.1180 - val_acc: 0.7514\n",
      "Epoch 49/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1117 - acc: 0.7685Epoch 00049: val_loss did not improve\n",
      "61597/61597 [==============================] - 874s 14ms/step - loss: 1.1115 - acc: 0.7685 - val_loss: 1.1179 - val_acc: 0.7157\n",
      "Epoch 50/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1111 - acc: 0.7685Epoch 00050: val_loss improved from 1.11759 to 1.11744, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 878s 14ms/step - loss: 1.1112 - acc: 0.7685 - val_loss: 1.1174 - val_acc: 0.7462\n",
      "Epoch 51/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1103 - acc: 0.7701Epoch 00051: val_loss improved from 1.11744 to 1.11719, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 878s 14ms/step - loss: 1.1102 - acc: 0.7701 - val_loss: 1.1172 - val_acc: 0.7384\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1095 - acc: 0.7694Epoch 00052: val_loss improved from 1.11719 to 1.11657, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 868s 14ms/step - loss: 1.1095 - acc: 0.7694 - val_loss: 1.1166 - val_acc: 0.7342\n",
      "Epoch 53/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1103 - acc: 0.7722Epoch 00053: val_loss did not improve\n",
      "61597/61597 [==============================] - 878s 14ms/step - loss: 1.1102 - acc: 0.7721 - val_loss: 1.1170 - val_acc: 0.7192\n",
      "Epoch 54/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1084 - acc: 0.7701Epoch 00054: val_loss improved from 1.11657 to 1.11551, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 874s 14ms/step - loss: 1.1084 - acc: 0.7701 - val_loss: 1.1155 - val_acc: 0.7292\n",
      "Epoch 55/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1083 - acc: 0.7691Epoch 00055: val_loss did not improve\n",
      "61597/61597 [==============================] - 875s 14ms/step - loss: 1.1081 - acc: 0.7691 - val_loss: 1.1155 - val_acc: 0.7487\n",
      "Epoch 56/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1071 - acc: 0.7715Epoch 00056: val_loss did not improve\n",
      "61597/61597 [==============================] - 874s 14ms/step - loss: 1.1071 - acc: 0.7714 - val_loss: 1.1180 - val_acc: 0.7352\n",
      "Epoch 57/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1068 - acc: 0.7692Epoch 00057: val_loss did not improve\n",
      "61597/61597 [==============================] - 873s 14ms/step - loss: 1.1068 - acc: 0.7692 - val_loss: 1.1165 - val_acc: 0.7122\n",
      "Epoch 58/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1057 - acc: 0.7686Epoch 00058: val_loss improved from 1.11551 to 1.11470, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 879s 14ms/step - loss: 1.1056 - acc: 0.7686 - val_loss: 1.1147 - val_acc: 0.7371\n",
      "Epoch 59/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1050 - acc: 0.7707Epoch 00059: val_loss did not improve\n",
      "61597/61597 [==============================] - 882s 14ms/step - loss: 1.1049 - acc: 0.7707 - val_loss: 1.1157 - val_acc: 0.7328\n",
      "Epoch 60/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1047 - acc: 0.7699Epoch 00060: val_loss did not improve\n",
      "61597/61597 [==============================] - 874s 14ms/step - loss: 1.1047 - acc: 0.7698 - val_loss: 1.1150 - val_acc: 0.7289\n",
      "Epoch 61/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1049 - acc: 0.7707Epoch 00061: val_loss did not improve\n",
      "61597/61597 [==============================] - 879s 14ms/step - loss: 1.1049 - acc: 0.7707 - val_loss: 1.1153 - val_acc: 0.7794\n",
      "Epoch 62/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1040 - acc: 0.7721Epoch 00062: val_loss did not improve\n",
      "61597/61597 [==============================] - 878s 14ms/step - loss: 1.1039 - acc: 0.7721 - val_loss: 1.1148 - val_acc: 0.7295\n",
      "Epoch 63/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1045 - acc: 0.7695Epoch 00063: val_loss improved from 1.11470 to 1.11383, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 882s 14ms/step - loss: 1.1045 - acc: 0.7695 - val_loss: 1.1138 - val_acc: 0.7523\n",
      "Epoch 64/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1026 - acc: 0.7710Epoch 00064: val_loss did not improve\n",
      "61597/61597 [==============================] - 875s 14ms/step - loss: 1.1027 - acc: 0.7709 - val_loss: 1.1147 - val_acc: 0.7436\n",
      "Epoch 65/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1019 - acc: 0.7696Epoch 00065: val_loss improved from 1.11383 to 1.11364, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 1040s 17ms/step - loss: 1.1017 - acc: 0.7696 - val_loss: 1.1136 - val_acc: 0.7756\n",
      "Epoch 66/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1026 - acc: 0.7697Epoch 00066: val_loss did not improve\n",
      "61597/61597 [==============================] - 1398s 23ms/step - loss: 1.1026 - acc: 0.7697 - val_loss: 1.1141 - val_acc: 0.7088\n",
      "Epoch 67/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1012 - acc: 0.7687Epoch 00067: val_loss improved from 1.11364 to 1.11267, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 1426s 23ms/step - loss: 1.1011 - acc: 0.7687 - val_loss: 1.1127 - val_acc: 0.7795\n",
      "Epoch 68/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1008 - acc: 0.7689Epoch 00068: val_loss did not improve\n",
      "61597/61597 [==============================] - 973s 16ms/step - loss: 1.1009 - acc: 0.7688 - val_loss: 1.1140 - val_acc: 0.7456\n",
      "Epoch 69/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.1009 - acc: 0.7683Epoch 00069: val_loss improved from 1.11267 to 1.11174, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 930s 15ms/step - loss: 1.1009 - acc: 0.7683 - val_loss: 1.1117 - val_acc: 0.7384\n",
      "Epoch 70/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0993 - acc: 0.7690Epoch 00070: val_loss did not improve\n",
      "61597/61597 [==============================] - 929s 15ms/step - loss: 1.0993 - acc: 0.7690 - val_loss: 1.1133 - val_acc: 0.7422\n",
      "Epoch 71/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0992 - acc: 0.7709Epoch 00071: val_loss did not improve\n",
      "61597/61597 [==============================] - 894s 15ms/step - loss: 1.0991 - acc: 0.7709 - val_loss: 1.1137 - val_acc: 0.7247\n",
      "Epoch 72/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0987 - acc: 0.7700Epoch 00072: val_loss improved from 1.11174 to 1.11025, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 880s 14ms/step - loss: 1.0987 - acc: 0.7700 - val_loss: 1.1103 - val_acc: 0.7659\n",
      "Epoch 73/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0984 - acc: 0.7698Epoch 00073: val_loss did not improve\n",
      "61597/61597 [==============================] - 879s 14ms/step - loss: 1.0983 - acc: 0.7698 - val_loss: 1.1109 - val_acc: 0.7402\n",
      "Epoch 74/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0992 - acc: 0.7701Epoch 00074: val_loss did not improve\n",
      "61597/61597 [==============================] - 888s 14ms/step - loss: 1.0991 - acc: 0.7700 - val_loss: 1.1112 - val_acc: 0.7646\n",
      "Epoch 75/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0975 - acc: 0.7699Epoch 00075: val_loss did not improve\n",
      "61597/61597 [==============================] - 883s 14ms/step - loss: 1.0975 - acc: 0.7698 - val_loss: 1.1105 - val_acc: 0.7890\n",
      "Epoch 76/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0977 - acc: 0.7697Epoch 00076: val_loss improved from 1.11025 to 1.10984, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 887s 14ms/step - loss: 1.0975 - acc: 0.7698 - val_loss: 1.1098 - val_acc: 0.7213\n",
      "Epoch 77/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0978 - acc: 0.7705Epoch 00077: val_loss improved from 1.10984 to 1.10928, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 901s 15ms/step - loss: 1.0978 - acc: 0.7706 - val_loss: 1.1093 - val_acc: 0.7682\n",
      "Epoch 78/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0966 - acc: 0.7701Epoch 00078: val_loss did not improve\n",
      "61597/61597 [==============================] - 910s 15ms/step - loss: 1.0964 - acc: 0.7701 - val_loss: 1.1093 - val_acc: 0.8002\n",
      "Epoch 79/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0966 - acc: 0.7705Epoch 00079: val_loss did not improve\n",
      "61597/61597 [==============================] - 928s 15ms/step - loss: 1.0965 - acc: 0.7706 - val_loss: 1.1097 - val_acc: 0.7317\n",
      "Epoch 80/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0962 - acc: 0.7696Epoch 00080: val_loss did not improve\n",
      "61597/61597 [==============================] - 890s 14ms/step - loss: 1.0961 - acc: 0.7696 - val_loss: 1.1117 - val_acc: 0.7043\n",
      "Epoch 81/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0954 - acc: 0.7710Epoch 00081: val_loss did not improve\n",
      "61597/61597 [==============================] - 886s 14ms/step - loss: 1.0954 - acc: 0.7710 - val_loss: 1.1098 - val_acc: 0.7192\n",
      "Epoch 82/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0946 - acc: 0.7685Epoch 00082: val_loss did not improve\n",
      "61597/61597 [==============================] - 884s 14ms/step - loss: 1.0946 - acc: 0.7685 - val_loss: 1.1103 - val_acc: 0.7370\n",
      "Epoch 83/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0938 - acc: 0.7687Epoch 00083: val_loss improved from 1.10928 to 1.10910, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 887s 14ms/step - loss: 1.0939 - acc: 0.7688 - val_loss: 1.1091 - val_acc: 0.7333\n",
      "Epoch 84/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0940 - acc: 0.7699Epoch 00084: val_loss did not improve\n",
      "61597/61597 [==============================] - 886s 14ms/step - loss: 1.0941 - acc: 0.7700 - val_loss: 1.1094 - val_acc: 0.7595\n",
      "Epoch 85/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0939 - acc: 0.7685Epoch 00085: val_loss did not improve\n",
      "61597/61597 [==============================] - 881s 14ms/step - loss: 1.0941 - acc: 0.7684 - val_loss: 1.1110 - val_acc: 0.7276\n",
      "Epoch 86/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0931 - acc: 0.7701Epoch 00086: val_loss did not improve\n",
      "61597/61597 [==============================] - 883s 14ms/step - loss: 1.0929 - acc: 0.7702 - val_loss: 1.1106 - val_acc: 0.7477\n",
      "Epoch 87/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0929 - acc: 0.7695Epoch 00087: val_loss did not improve\n",
      "61597/61597 [==============================] - 895s 15ms/step - loss: 1.0927 - acc: 0.7695 - val_loss: 1.1095 - val_acc: 0.7570\n",
      "Epoch 88/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0935 - acc: 0.7703Epoch 00088: val_loss did not improve\n",
      "61597/61597 [==============================] - 971s 16ms/step - loss: 1.0938 - acc: 0.7702 - val_loss: 1.1101 - val_acc: 0.7369\n",
      "Epoch 89/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0934 - acc: 0.7688Epoch 00089: val_loss did not improve\n",
      "61597/61597 [==============================] - 1068s 17ms/step - loss: 1.0933 - acc: 0.7688 - val_loss: 1.1092 - val_acc: 0.7311\n",
      "Epoch 90/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0911 - acc: 0.7705Epoch 00090: val_loss did not improve\n",
      "61597/61597 [==============================] - 919s 15ms/step - loss: 1.0911 - acc: 0.7705 - val_loss: 1.1093 - val_acc: 0.7147\n",
      "Epoch 91/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0897 - acc: 0.7696Epoch 00091: val_loss did not improve\n",
      "61597/61597 [==============================] - 996s 16ms/step - loss: 1.0895 - acc: 0.7697 - val_loss: 1.1098 - val_acc: 0.8080\n",
      "Epoch 92/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0918 - acc: 0.7686Epoch 00092: val_loss improved from 1.10910 to 1.10892, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 1165s 19ms/step - loss: 1.0918 - acc: 0.7686 - val_loss: 1.1089 - val_acc: 0.7625\n",
      "Epoch 93/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0915 - acc: 0.7682Epoch 00093: val_loss did not improve\n",
      "61597/61597 [==============================] - 1192s 19ms/step - loss: 1.0916 - acc: 0.7682 - val_loss: 1.1099 - val_acc: 0.7236\n",
      "Epoch 94/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0916 - acc: 0.7689Epoch 00094: val_loss improved from 1.10892 to 1.10782, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 954s 15ms/step - loss: 1.0916 - acc: 0.7689 - val_loss: 1.1078 - val_acc: 0.7463\n",
      "Epoch 95/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0901 - acc: 0.7678Epoch 00095: val_loss did not improve\n",
      "61597/61597 [==============================] - 956s 16ms/step - loss: 1.0900 - acc: 0.7678 - val_loss: 1.1081 - val_acc: 0.7528\n",
      "Epoch 96/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0901 - acc: 0.7683Epoch 00096: val_loss did not improve\n",
      "61597/61597 [==============================] - 909s 15ms/step - loss: 1.0903 - acc: 0.7683 - val_loss: 1.1087 - val_acc: 0.7801\n",
      "Epoch 97/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0911 - acc: 0.7691Epoch 00097: val_loss did not improve\n",
      "61597/61597 [==============================] - 900s 15ms/step - loss: 1.0909 - acc: 0.7691 - val_loss: 1.1084 - val_acc: 0.7487\n",
      "Epoch 98/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0893 - acc: 0.7700Epoch 00098: val_loss did not improve\n",
      "61597/61597 [==============================] - 885s 14ms/step - loss: 1.0892 - acc: 0.7701 - val_loss: 1.1083 - val_acc: 0.7559\n",
      "Epoch 99/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0892 - acc: 0.7698Epoch 00099: val_loss did not improve\n",
      "61597/61597 [==============================] - 881s 14ms/step - loss: 1.0892 - acc: 0.7698 - val_loss: 1.1095 - val_acc: 0.7722\n",
      "Epoch 100/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0902 - acc: 0.7685Epoch 00100: val_loss did not improve\n",
      "61597/61597 [==============================] - 881s 14ms/step - loss: 1.0903 - acc: 0.7684 - val_loss: 1.1079 - val_acc: 0.7306\n",
      "Epoch 101/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0883 - acc: 0.7704Epoch 00101: val_loss did not improve\n",
      "61597/61597 [==============================] - 880s 14ms/step - loss: 1.0882 - acc: 0.7703 - val_loss: 1.1088 - val_acc: 0.6930\n",
      "Epoch 102/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0888 - acc: 0.7710Epoch 00102: val_loss improved from 1.10782 to 1.10676, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 882s 14ms/step - loss: 1.0888 - acc: 0.7710 - val_loss: 1.1068 - val_acc: 0.7415\n",
      "Epoch 103/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0867 - acc: 0.7700Epoch 00103: val_loss did not improve\n",
      "61597/61597 [==============================] - 871s 14ms/step - loss: 1.0869 - acc: 0.7700 - val_loss: 1.1081 - val_acc: 0.7403\n",
      "Epoch 104/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0880 - acc: 0.7698Epoch 00104: val_loss did not improve\n",
      "61597/61597 [==============================] - 876s 14ms/step - loss: 1.0880 - acc: 0.7698 - val_loss: 1.1075 - val_acc: 0.7706\n",
      "Epoch 105/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0887 - acc: 0.7687Epoch 00105: val_loss did not improve\n",
      "61597/61597 [==============================] - 867s 14ms/step - loss: 1.0887 - acc: 0.7686 - val_loss: 1.1070 - val_acc: 0.7252\n",
      "Epoch 106/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0875 - acc: 0.7692Epoch 00106: val_loss did not improve\n",
      "61597/61597 [==============================] - 884s 14ms/step - loss: 1.0874 - acc: 0.7692 - val_loss: 1.1089 - val_acc: 0.7056\n",
      "Epoch 107/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0867 - acc: 0.7703Epoch 00107: val_loss did not improve\n",
      "61597/61597 [==============================] - 887s 14ms/step - loss: 1.0868 - acc: 0.7703 - val_loss: 1.1075 - val_acc: 0.7192\n",
      "Epoch 108/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0865 - acc: 0.7699Epoch 00108: val_loss did not improve\n",
      "61597/61597 [==============================] - 873s 14ms/step - loss: 1.0866 - acc: 0.7700 - val_loss: 1.1083 - val_acc: 0.7146\n",
      "Epoch 109/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0862 - acc: 0.7699Epoch 00109: val_loss improved from 1.10676 to 1.10623, saving model to model/cnn_hashtagcls_emb200_weights1biLSTM.h5\n",
      "61597/61597 [==============================] - 869s 14ms/step - loss: 1.0865 - acc: 0.7699 - val_loss: 1.1062 - val_acc: 0.7524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0872 - acc: 0.7702Epoch 00110: val_loss did not improve\n",
      "61597/61597 [==============================] - 869s 14ms/step - loss: 1.0874 - acc: 0.7702 - val_loss: 1.1082 - val_acc: 0.7186\n",
      "Epoch 111/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0869 - acc: 0.7706Epoch 00111: val_loss did not improve\n",
      "61597/61597 [==============================] - 863s 14ms/step - loss: 1.0867 - acc: 0.7706 - val_loss: 1.1086 - val_acc: 0.7247\n",
      "Epoch 112/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0851 - acc: 0.7687Epoch 00112: val_loss did not improve\n",
      "61597/61597 [==============================] - 870s 14ms/step - loss: 1.0853 - acc: 0.7687 - val_loss: 1.1077 - val_acc: 0.7502\n",
      "Epoch 113/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0860 - acc: 0.7687Epoch 00113: val_loss did not improve\n",
      "61597/61597 [==============================] - 865s 14ms/step - loss: 1.0861 - acc: 0.7686 - val_loss: 1.1099 - val_acc: 0.7037\n",
      "Epoch 114/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0859 - acc: 0.7703Epoch 00114: val_loss did not improve\n",
      "61597/61597 [==============================] - 864s 14ms/step - loss: 1.0858 - acc: 0.7703 - val_loss: 1.1076 - val_acc: 0.7595\n",
      "Epoch 115/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0844 - acc: 0.7701Epoch 00115: val_loss did not improve\n",
      "61597/61597 [==============================] - 867s 14ms/step - loss: 1.0846 - acc: 0.7701 - val_loss: 1.1084 - val_acc: 0.7647\n",
      "Epoch 116/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0847 - acc: 0.7694Epoch 00116: val_loss did not improve\n",
      "61597/61597 [==============================] - 866s 14ms/step - loss: 1.0845 - acc: 0.7694 - val_loss: 1.1068 - val_acc: 0.7624\n",
      "Epoch 117/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0843 - acc: 0.7692Epoch 00117: val_loss did not improve\n",
      "61597/61597 [==============================] - 863s 14ms/step - loss: 1.0843 - acc: 0.7691 - val_loss: 1.1070 - val_acc: 0.7374\n",
      "Epoch 118/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0846 - acc: 0.7700Epoch 00118: val_loss did not improve\n",
      "61597/61597 [==============================] - 860s 14ms/step - loss: 1.0845 - acc: 0.7699 - val_loss: 1.1070 - val_acc: 0.7097\n",
      "Epoch 119/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 1.0831 - acc: 0.7683Epoch 00119: val_loss did not improve\n",
      "61597/61597 [==============================] - 863s 14ms/step - loss: 1.0831 - acc: 0.7684 - val_loss: 1.1073 - val_acc: 0.7222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f0865d6fd0>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.optimizer.lr=1e-4\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "# early_stopping = EarlyStopping(monitor='val_acc', patience=10)\n",
    "\n",
    "sub = \"1biLSTM\"\n",
    "model_chk_path = 'model/cnn_hashtagcls_emb{}_weights{}.h5'.format(embedding_dim, sub)\n",
    "mcp = ModelCheckpoint(model_chk_path, monitor=\"val_loss\", verbose=1,\n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "model.fit(X_train_u, y_train, validation_data=(X_test_u, y_test), epochs=500, batch_size=64, callbacks=[early_stopping, mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2939,\n",
       " 1: 2368,\n",
       " 2: 14558,\n",
       " 3: 2724,\n",
       " 4: 12978,\n",
       " 5: 20715,\n",
       " 6: 13457,\n",
       " 7: 2458,\n",
       " 8: 3738,\n",
       " 9: 52454,\n",
       " 10: 2196}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#class_weight\n",
    "cw_map = {'cancer': 4,\n",
    " 'cervicalcanc': 6,\n",
    " 'gardasil': 2,\n",
    " 'health': 0,\n",
    " 'hpv': 9,\n",
    " 'hpvvaccin': 3,\n",
    " 'learntherisk': 8,\n",
    " 'studi': 7,\n",
    " 'vaccin': 5,\n",
    " 'vaccineswork': 10,\n",
    " 'vax': 1}\n",
    "\n",
    "cw = [('hpv', 52454),\n",
    " ('vaccin', 20715),\n",
    " ('gardasil', 14558),\n",
    " ('cervicalcanc', 13457),\n",
    " ('cancer', 12978),\n",
    " ('learntherisk', 3738),\n",
    " ('health', 2939),\n",
    " ('hpvvaccin', 2724),\n",
    " ('studi', 2458),\n",
    " ('vax', 2368),\n",
    " ('vaccineswork', 2196)]\n",
    "\n",
    "class_weight = dict()\n",
    "\n",
    "for each in cw :\n",
    "    class_weight[cw_map[each[0]]] = each[1]\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61597 samples, validate on 16210 samples\n",
      "Epoch 1/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.8347 - acc: 0.6914Epoch 00001: val_loss did not improve\n",
      "61597/61597 [==============================] - 307s 5ms/step - loss: 0.8345 - acc: 0.6914 - val_loss: 0.4316 - val_acc: 0.8578\n",
      "Epoch 2/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.3395 - acc: 0.8861Epoch 00002: val_loss did not improve\n",
      "61597/61597 [==============================] - 270s 4ms/step - loss: 0.3395 - acc: 0.8861 - val_loss: 0.3134 - val_acc: 0.8897\n",
      "Epoch 3/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9251Epoch 00003: val_loss did not improve\n",
      "61597/61597 [==============================] - 278s 5ms/step - loss: 0.2250 - acc: 0.9251 - val_loss: 0.2898 - val_acc: 0.9068\n",
      "Epoch 4/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9498Epoch 00004: val_loss did not improve\n",
      "61597/61597 [==============================] - 305s 5ms/step - loss: 0.1572 - acc: 0.9498 - val_loss: 0.2940 - val_acc: 0.9123\n",
      "Epoch 5/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9600Epoch 00005: val_loss did not improve\n",
      "61597/61597 [==============================] - 289s 5ms/step - loss: 0.1256 - acc: 0.9599 - val_loss: 0.3136 - val_acc: 0.9141\n",
      "Epoch 6/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9679Epoch 00006: val_loss did not improve\n",
      "61597/61597 [==============================] - 269s 4ms/step - loss: 0.1026 - acc: 0.9679 - val_loss: 0.3879 - val_acc: 0.9213\n",
      "Epoch 7/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9736Epoch 00007: val_loss did not improve\n",
      "61597/61597 [==============================] - 271s 4ms/step - loss: 0.0849 - acc: 0.9736 - val_loss: 0.3333 - val_acc: 0.9218\n",
      "Epoch 8/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9759Epoch 00008: val_loss did not improve\n",
      "61597/61597 [==============================] - 272s 4ms/step - loss: 0.0792 - acc: 0.9759 - val_loss: 0.3835 - val_acc: 0.9230\n",
      "Epoch 9/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9789Epoch 00009: val_loss did not improve\n",
      "61597/61597 [==============================] - 270s 4ms/step - loss: 0.0670 - acc: 0.9789 - val_loss: 0.3448 - val_acc: 0.9197\n",
      "Epoch 10/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9814Epoch 00010: val_loss did not improve\n",
      "61597/61597 [==============================] - 269s 4ms/step - loss: 0.0619 - acc: 0.9814 - val_loss: 0.4551 - val_acc: 0.9150\n",
      "Epoch 11/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9815Epoch 00011: val_loss did not improve\n",
      "61597/61597 [==============================] - 267s 4ms/step - loss: 0.0590 - acc: 0.9815 - val_loss: 0.3855 - val_acc: 0.9181\n",
      "Epoch 12/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9828Epoch 00012: val_loss did not improve\n",
      "61597/61597 [==============================] - 267s 4ms/step - loss: 0.0574 - acc: 0.9828 - val_loss: 0.4268 - val_acc: 0.9206\n",
      "Epoch 13/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9845Epoch 00013: val_loss did not improve\n",
      "61597/61597 [==============================] - 267s 4ms/step - loss: 0.0514 - acc: 0.9845 - val_loss: 0.4782 - val_acc: 0.9235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c140088b38>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_u, y_train, validation_data=(X_test_u, y_test), epochs=500, batch_size=64, callbacks=[mcp, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### best result\n",
    "- 0.8402"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### current best model snapshot:\n",
    "\n",
    "```python\n",
    "dth = 0.5\n",
    "filter_num = 5\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#embedding layer\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=seq_len, weights=[embedding], trainable=True))\n",
    "\n",
    "#hidden layer\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Dropout(dth))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Dropout(dth))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Dropout(dth))\n",
    "\n",
    "#output layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(modeling_data['categorical_num'], activation='sigmoid'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single label model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     2     0    46   781    22    74\n",
      "    100   121     7    13    35    55   352    54    40   162    15     1\n",
      "      3     4  1126]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0   364    93     5\n",
      "  10430  1019    20  1770  3765    22     1     3  1637    12     9    61\n",
      "     10  8726    18]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0  1159    49\n",
      "    251    22    97  4382    12   448  3863     8    20     1    25   493\n",
      "     18   400    13]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     2     0   122   312    90   662    57     8\n",
      "    369    33    13     4   529    26   967    16  1777   831     8    39\n",
      "    365    12     1]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      2     0    27     0    11   131    19   328    10    31   244    54\n",
      "     40     1    51]]\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0  210  338   41  215  165\n",
      "    34    1  471    4  459  182  409   29   20  411    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0   13  180   57 1813 2238  296\n",
      "     5 1477 1003  134    6 3875  616    8  448   67    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    2    0   27    0   14  262    7  346\n",
      "   362   10   14  509  110  118  250   34  168    1    3]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0  726   12 8803  423\n",
      "    23  155   15  281  223  397    8    1    0  492  287]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0   97    1   16    9  153 2965\n",
      "  3326 3551   22 2585  193   18 1077  125    8  159    0]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     3  1413   264    41   177     1    68    32\n",
      "    757    37   697]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     2     0     4    84\n",
      "    981    18    11  3379     6   226  2928 10352  1760    81 10353     6\n",
      "   4817 19993     4]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     2     0    30    24   478\n",
      "     11 11792    21]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     1    25    68   569   145   917    41    10\n",
      "   2948  1620    45]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      2     0  1026     0  2602  1592    16  1282  4158     7    19     9\n",
      "    422    33  1780]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#reformat the data\n",
    "'''\n",
    "{0: 2939,\n",
    " 1: 2368,\n",
    " 2: 14558,\n",
    " 3: 2724,\n",
    " 4: 12978,\n",
    " 5: 20715,\n",
    " 6: 13457,\n",
    " 7: 2458,\n",
    " 8: 3738,\n",
    " 9: 52454,\n",
    " 10: 2196}\n",
    "'''\n",
    "iclass_weight = {v: k for k, v in class_weight.items()}\n",
    "\n",
    "def to_single_categorical_label(y):\n",
    "    res = []\n",
    "    for each in y:\n",
    "        res.append(iclass_weight[max([class_weight[e] for e in each])])\n",
    "    return to_categorical(res, num_classes=modeling_data['categorical_num'])\n",
    "\n",
    "n_y = to_single_categorical_label(label)\n",
    "\n",
    "X_train_1, X_test, y_train_1, y_test = train_test_split(data, n_y, test_size=0.2, random_state=42)\n",
    "\n",
    "cut = int(len(X_train_1) * 0.95)\n",
    "X_train = X_train_1[:cut]\n",
    "y_train = y_train_1[:cut]\n",
    "X_pred = X_train_1[cut:]\n",
    "y_pred = y_train_1[cut:]\n",
    "\n",
    "X_train_f = [[dictionary.index(word) if word in dictionary else -1 for word in doc] for doc in X_train]\n",
    "X_test_f = [[dictionary.index(word) if word in dictionary else -1 for word in doc] for doc in X_test]\n",
    "X_pred_f = [[dictionary.index(word) if word in dictionary else -1 for word in doc] for doc in X_pred]\n",
    "\n",
    "X_train_u = sequence.pad_sequences(X_train_f, maxlen=seq_len)\n",
    "X_test_u = sequence.pad_sequences(X_test_f, maxlen=seq_len)\n",
    "X_pred_u = sequence.pad_sequences(X_pred_f, maxlen=seq_len)\n",
    "\n",
    "print(X_train_u[:5])\n",
    "print(X_test_u[:5])\n",
    "print(X_pred_u[:5])\n",
    "print(y_train[:5])\n",
    "print(y_pred[:5])\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 39, 100)           2330700   \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 39, 100)           50100     \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 39, 100)           50100     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 39, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 39, 100)           50100     \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 39, 100)           50100     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 39, 100)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 3900)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 11)                42911     \n",
      "=================================================================\n",
      "Total params: 2,574,011\n",
      "Trainable params: 2,574,011\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 61597 samples, validate on 16210 samples\n",
      "Epoch 1/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.7211 - acc: 0.7164Epoch 00001: val_loss did not improve\n",
      "61597/61597 [==============================] - 216s 4ms/step - loss: 0.7211 - acc: 0.7163 - val_loss: 0.3955 - val_acc: 0.8561\n",
      "Epoch 2/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.8999Epoch 00002: val_loss improved from 0.28064 to 0.26513, saving model to model/cnn_hashtagcls_emb100_weights.validation.h5\n",
      "61597/61597 [==============================] - 226s 4ms/step - loss: 0.2794 - acc: 0.8998 - val_loss: 0.2651 - val_acc: 0.9034\n",
      "Epoch 3/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9370Epoch 00003: val_loss improved from 0.26513 to 0.26168, saving model to model/cnn_hashtagcls_emb100_weights.validation.h5\n",
      "61597/61597 [==============================] - 216s 4ms/step - loss: 0.1794 - acc: 0.9370 - val_loss: 0.2617 - val_acc: 0.9176\n",
      "Epoch 4/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9560Epoch 00004: val_loss did not improve\n",
      "61597/61597 [==============================] - 216s 4ms/step - loss: 0.1268 - acc: 0.9560 - val_loss: 0.2731 - val_acc: 0.9126\n",
      "Epoch 5/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9671Epoch 00005: val_loss did not improve\n",
      "61597/61597 [==============================] - 218s 4ms/step - loss: 0.0954 - acc: 0.9671 - val_loss: 0.3174 - val_acc: 0.9217\n",
      "Epoch 6/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9740Epoch 00006: val_loss did not improve\n",
      "61597/61597 [==============================] - 217s 4ms/step - loss: 0.0758 - acc: 0.9740 - val_loss: 0.3431 - val_acc: 0.9181\n",
      "Epoch 7/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9782Epoch 00007: val_loss did not improve\n",
      "61597/61597 [==============================] - 218s 4ms/step - loss: 0.0624 - acc: 0.9782 - val_loss: 0.4477 - val_acc: 0.9165\n",
      "Epoch 8/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9788Epoch 00008: val_loss did not improve\n",
      "61597/61597 [==============================] - 218s 4ms/step - loss: 0.0607 - acc: 0.9788 - val_loss: 0.3533 - val_acc: 0.9176\n",
      "Epoch 9/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9815Epoch 00009: val_loss did not improve\n",
      "61597/61597 [==============================] - 219s 4ms/step - loss: 0.0554 - acc: 0.9815 - val_loss: 0.4775 - val_acc: 0.9189\n",
      "Epoch 10/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9835Epoch 00010: val_loss did not improve\n",
      "61597/61597 [==============================] - 222s 4ms/step - loss: 0.0468 - acc: 0.9835 - val_loss: 0.4472 - val_acc: 0.9190\n",
      "Epoch 11/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9846Epoch 00011: val_loss did not improve\n",
      "61597/61597 [==============================] - 221s 4ms/step - loss: 0.0459 - acc: 0.9846 - val_loss: 0.4223 - val_acc: 0.9225\n",
      "Epoch 12/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9851Epoch 00012: val_loss did not improve\n",
      "61597/61597 [==============================] - 220s 4ms/step - loss: 0.0427 - acc: 0.9851 - val_loss: 0.4502 - val_acc: 0.9212\n",
      "Epoch 13/500\n",
      "61568/61597 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9853Epoch 00013: val_loss did not improve\n",
      "61597/61597 [==============================] - 220s 4ms/step - loss: 0.0430 - acc: 0.9853 - val_loss: 0.4932 - val_acc: 0.9175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c13d917a20>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dth = 0.5\n",
    "filter_num = 5\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#embedding layer\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=seq_len, weights=[embedding], trainable=True))\n",
    "# model.add(Dropout(dth))\n",
    "\n",
    "#hidden layer\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Dropout(dth))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Dropout(dth))\n",
    "\n",
    "#output layer\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(dth))\n",
    "model.add(Dense(modeling_data['categorical_num'], activation='sigmoid'))\n",
    "\n",
    "#compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train_u, y_train, validation_data=(X_test_u, y_test), epochs=500, batch_size=64, callbacks=[mcp, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and save model for future usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the model\n",
    "model.save_weights('model/cnn_hashtagcls_emb{}_weights.h5'.format(embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine train and test data and retrain the model as the final model to use for predication\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5)\n",
    "model.fit(\n",
    "    np.concatenate((X_train_u, y_train), axis=0),\n",
    "    np.concatenate((X_test_u, y_test), axis=0),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping])\n",
    "model.save_weights(\n",
    "    \"model/all_cnn_hashtagcls_emb{}_weights.h5\".format(embedding_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract the last hidden layer information\n",
    ">https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Dense.__dir__>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predication\n",
    "**The predication will be performed on both no_labeled data set and neg_sample and pred set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load two data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "with open(\"temp/neg_sample_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    neg_sample = json.load(f)\n",
    "    \n",
    "with open(\"temp/no_labeled_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    nolabel_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    3 1413\n",
      "  264   41  177    1   68   32  757   37  697]\n",
      "3242\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "3242\n"
     ]
    }
   ],
   "source": [
    "print(X_pred_u[0])\n",
    "print(len(X_pred_u))\n",
    "print(y_pred[0])\n",
    "print(len(y_pred))\n",
    "pred_set = list(zip(X_pred_u, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_map = {'cancer': 4,\n",
    " 'cervicalcanc': 6,\n",
    " 'gardasil': 2,\n",
    " 'health': 0,\n",
    " 'hpv': 9,\n",
    " 'hpvvaccin': 3,\n",
    " 'learntherisk': 8,\n",
    " 'studi': 7,\n",
    " 'vaccin': 5,\n",
    " 'vaccineswork': 10,\n",
    " 'vax': 1}\n",
    "\n",
    "\n",
    "\n",
    "classes_decoder = {v : k for k, v in cw_map.items()}\n",
    "words_decoder = dict()\n",
    "for i, each in enumerate(dictionary):\n",
    "    words_decoder[i] = each\n",
    "# print(classes_decoder)\n",
    "# print(words_decoder)\n",
    "\n",
    "def decode_classes(data):\n",
    "    true_label = []\n",
    "    for i, each in enumerate(data):\n",
    "        if int(each):\n",
    "            true_label.append(classes_decoder[i])\n",
    "    return true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(['vaccine', 'cuts', 'teen', 'girls', \"'\", 'hpv', 'rates', 'by', 'two-thirds', 'health', 'living'], array([  9.89149749e-01,   8.59721843e-03,   2.24388077e-06,\n",
    "         3.98910970e-06,   9.51678339e-06,   2.09738361e-03,\n",
    "         2.01194634e-08,   5.34608091e-07,   1.39307536e-04,\n",
    "         5.87877942e-08,   3.33531175e-10], dtype=float32), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.89149749e-01   8.59721843e-03   2.24388077e-06   3.98910970e-06\n",
      "   9.51678339e-06   2.09738361e-03   2.01194634e-08   5.34608091e-07\n",
      "   1.39307536e-04   5.87877942e-08   3.33531175e-10]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_pred_u)[0])\n",
    "print(model.predict_classes(X_pred_u)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import csv\n",
    "import operator\n",
    "\n",
    "subs = [\"2cnn+max+drop\"]\n",
    "for sub in subs:\n",
    "    model_chk_path = 'model/cnn_hashtagcls_emb{}_weights{}.h5'.format(embedding_dim, sub)\n",
    "    model = load_model(model_chk_path)\n",
    "    \n",
    "    pred_prob = model.predict(X_pred_u)\n",
    "#     pred_class = model.predict_classes(X_pred_u)\n",
    "    \n",
    "    res = list(zip(X_pred, pred_prob, y_pred))\n",
    "    \n",
    "    output_file = \"predict_res/predict_results_\" + sub + \".csv\"\n",
    "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as fw:\n",
    "        header = [\"tweet_text\", \"true_label\"]\n",
    "        header.extend([each[0] for each in sorted(cw_map.items(), key=operator.itemgetter(1))])\n",
    "        writer = csv.DictWriter(fw, fieldnames=header)  \n",
    "        writer.writeheader()\n",
    "        for each in res:    \n",
    "            tweet_text = \" \".join(each[0])\n",
    "            tweet_true_label = \", \".join(decode_classes(each[2]))\n",
    "            pred_prob = each[1]\n",
    "            info = [tweet_text, tweet_true_label]\n",
    "            info.extend(pred_prob)\n",
    "            res = {each[0]:each[1] for each in zip(header, info)}\n",
    "            writer.writerow(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_f = [[dictionary.index(word) if word in dictionary else -1 for word in doc] for doc in X_train]\n",
    "# X_train_u = sequence.pad_sequences(X_train_f, maxlen=seq_len)\n",
    "\n",
    "X_pred_no_label_o = []\n",
    "\n",
    "for each in nolabel_sample['data']:\n",
    "    X_pred_no_label_o.append(each['words'])\n",
    "\n",
    "X_pred_no_label = [[dictionary.index(word) if word in dictionary else 0 for word in doc] for doc in X_pred_no_label_o] \n",
    "X_pred_no_label = sequence.pad_sequences(X_pred_no_label, maxlen=seq_len)\n",
    "X_pred_no_label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import csv\n",
    "import operator\n",
    "\n",
    "subs = [\"2cnn+max+drop\"]\n",
    "for sub in subs:\n",
    "    model_chk_path = 'model/cnn_hashtagcls_emb{}_weights{}.h5'.format(embedding_dim, sub)\n",
    "    model = load_model(model_chk_path)\n",
    "    \n",
    "    pred_prob = model.predict(X_pred_no_label)\n",
    "#     pred_class = model.predict_classes(X_pred_no_label)\n",
    "    \n",
    "    res = list(zip(X_pred_no_label_o, pred_prob))\n",
    "    \n",
    "    output_file = \"predict_res/predict_results_on_no_label_sample\" + sub + \".csv\"\n",
    "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as fw:\n",
    "        header = [\"tweet_text\", \"true_label\"]\n",
    "        header.extend([each[0] for each in sorted(cw_map.items(), key=operator.itemgetter(1))])\n",
    "        writer = csv.DictWriter(fw, fieldnames=header)  \n",
    "        writer.writeheader()\n",
    "        for each in res:    \n",
    "            tweet_text = \" \".join(each[0])\n",
    "            tweet_true_label = \"no label\"#\", \".join(decode_classes(each[2]))\n",
    "            pred_prob = each[1]\n",
    "            info = [tweet_text, tweet_true_label]\n",
    "            info.extend(pred_prob)\n",
    "            res = {each[0]:each[1] for each in zip(header, info)}\n",
    "            writer.writerow(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
