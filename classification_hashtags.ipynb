{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets classification by its hashtags and no labeled tweets hashtag predication\n",
    "\n",
    "\n",
    "**Using top n hashtags as label to build a supervised model for tweets classification and hashtag predication**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load packages and modeling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1727432216832600299\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow backend\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.8\n",
      "tensorflow\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import *\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "print(keras.__version__)\n",
    "print(keras.backend.backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bcolz\n",
    "import pickle\n",
    "\n",
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "    \n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]\n",
    "\n",
    "def save_dict(fname, dictionary):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(dictionary, f)\n",
    "\n",
    "def load_dict(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hashtag_label': [1, 5],\n",
       " 'hashtags': ['hpv', 'vaccin'],\n",
       " 'id': '418263863772327936',\n",
       " 'orignal_hashtags': ['#hpv', '#vaccine'],\n",
       " 'raw': 'rt @cdcstd: #hpv vax coverage could be 93% if doctors gave hpv #vaccine each time a preteen/teen got any other vaccine&gt; http://t.co/xxryga5…',\n",
       " 'text': 'rt : hpv vax coverage could be 93% if doctors gave hpv vaccine each time a preteen / teen got any other vaccine>',\n",
       " 'words': ['rt',\n",
       "  ':',\n",
       "  'hpv',\n",
       "  'vax',\n",
       "  'coverage',\n",
       "  'could',\n",
       "  'be',\n",
       "  '93',\n",
       "  '%',\n",
       "  'if',\n",
       "  'doctors',\n",
       "  'gave',\n",
       "  'hpv',\n",
       "  'vaccine',\n",
       "  'each',\n",
       "  'time',\n",
       "  'a',\n",
       "  'preteen',\n",
       "  '/',\n",
       "  'teen',\n",
       "  'got',\n",
       "  'any',\n",
       "  'other',\n",
       "  'vaccine',\n",
       "  '>']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load tweets data\n",
    "import json\n",
    "tweets_file = \"temp/tweets4classification.json\"\n",
    "with open(tweets_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    modeling_data = json.load(f)\n",
    "modeling_data['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load processed word enbeddings\n",
    "path = 'wordsenbeddings/'\n",
    "res_path = path + 'results/'\n",
    "\n",
    "def load_vectors(name):\n",
    "    loc = res_path + name\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_glove(name, dim):\n",
    "    with open(path+ 'glove.' + name + '.txt', 'r', encoding=\"utf-8\") as f:\n",
    "        vecs = []\n",
    "        words = []\n",
    "        \n",
    "        for i, line in enumerate(f):\n",
    "            d = line.split()\n",
    "            word = d[0]\n",
    "            vec = np.array(d[1:], dtype=np.float32)\n",
    "            if (len(d) == dim): # this is space\n",
    "                word = ' '\n",
    "                vec = np.array(d, dtype=np.float32)\n",
    "            \n",
    "            words.append(word)            \n",
    "            vecs.append(vec)\n",
    "\n",
    "        wordidx = {o:i for i,o in enumerate(words)}\n",
    "        save_array(res_path+name+'.dat', vecs)\n",
    "        pickle.dump(words, open(res_path+name+'_words.pkl','wb'))\n",
    "        pickle.dump(wordidx, open(res_path+name+'_idx.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_glove('twitter.27B.200d', 200)\n",
    "get_glove('twitter.27B.25d', 25)\n",
    "get_glove('twitter.27B.50d', 50)\n",
    "get_glove('twitter.27B.100d', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare train and test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ list(['rt', ':', 'hpv', 'vax', 'coverage', 'could', 'be', '93', '%', 'if', 'doctors', 'gave', 'hpv', 'vaccine', 'each', 'time', 'a', 'preteen', '/', 'teen', 'got', 'any', 'other', 'vaccine', '>'])\n",
      " list(['rt', ':', 'hpv', 'vax', 'coverage', 'could', 'be', '93', '%', 'if', 'doctors', 'gave', 'hpv', 'vaccine', 'each', 'time', 'a', 'preteen', '/', 'teen', 'got', 'any', 'other', 'vaccine', '...', '.'])]\n",
      "[list([1, 5]) list([1, 5])]\n",
      "81049\n",
      "81049\n"
     ]
    }
   ],
   "source": [
    "data = np.asarray([each['words'] for each in modeling_data['data']])\n",
    "label = np.asarray([each['hashtag_label'] for each in modeling_data['data']])\n",
    "\n",
    "print(data[:2])\n",
    "print(label[:2])\n",
    "print(len(data))\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flat_labels(labels):\n",
    "    flatted = []\n",
    "    l = modeling_data['categorical_num']\n",
    "    for label in labels:\n",
    "        m = [0.] * l\n",
    "        for each in label:\n",
    "            m = list(map(lambda x: x[0] + x[1], zip(m, each)))\n",
    "        flatted.append(m)\n",
    "    return np.asarray(flatted)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "categorical_label = list(map(lambda x: to_categorical(x, num_classes=modeling_data['categorical_num']), label))\n",
    "\n",
    "categorical_label_flatted = flat_labels(categorical_label)\n",
    "\n",
    "print(len(categorical_label))\n",
    "categorical_label_flatted[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_0, X_test, y_train_0, y_test = train_test_split(data, categorical_label_flatted, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ list(['rt', ':', 'hpv', 'fact', ':', 'hpv', 'is', 'the', 'primary', 'cause', 'of', 'cervicalcancer', ',', 'certain', 'types', 'of', 'head', '&', 'neck', 'cancer', ',', 'in', 'addition', 'to', 'several', 'rare', 'cance'])\n",
      " list(['rt', ':', 'study', ':', 'hpv', 'vaccine', 'linked', 'to', 'premature', 'menopause', 'in', 'young', 'girls'])]\n",
      "[ list(['two', 'uk', 'girls', 'left', 'paralyzed', 'after', 'hpv', 'jabs', '.', 'authorities', 'still', 'claim', 'it', \"'s\", 'coincidence', '.'])\n",
      " list(['cervicalcancer', 'deaths', 'have', 'decreased', 'dramatically', 'over', 'the', 'past', '40', 'years', ',', 'mostly', 'due', 'to', 'increased', 'screening', '.'])]\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.]]\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_0[:2])\n",
    "print(X_test[:2])\n",
    "print(y_train_0[:2])\n",
    "print(y_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save 5% train samples to predication task\n",
    "cut = int(len(X_train_0) * 0.95)\n",
    "X_train = X_train_0[:cut]\n",
    "y_train = y_train_0[:cut]\n",
    "X_pred = X_train_0[cut:]\n",
    "y_pred = y_train_0[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-8b9e755e8c1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfrequency\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mall_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodeling_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-8b9e755e8c1c>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfrequency\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mall_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodeling_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!!!!!do not run this, run the third one below this to directly load the dictionary\n",
    "#create words dictionary for the data\n",
    "from functools import reduce\n",
    "\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "all_tokens = list(reduce(lambda x, y: x + y, [l['words'] for l in modeling_data['data']]))\n",
    "print(all_tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_tokens))\n",
    "vocab_size = 0\n",
    "\n",
    "for token in all_tokens:\n",
    "    frequency[token] += 1\n",
    "    \n",
    "dictionary = sorted(frequency.items(), key=lambda x:x[1], reverse=True)\n",
    "if (not vocab_size):\n",
    "    vocab_size = len(dictionary)\n",
    "dictionary = [k for k,v in dictionary[:vocab_size]]\n",
    "print(dictionary[:10])\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dict('model/dict.dd', dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':',\n",
       " 'hpv',\n",
       " 'rt',\n",
       " 'vaccine',\n",
       " '.',\n",
       " 'the',\n",
       " ',',\n",
       " 'of',\n",
       " 'to',\n",
       " 'cancer',\n",
       " 'in',\n",
       " 'gardasil',\n",
       " 'for',\n",
       " 'cervicalcancer',\n",
       " 'a',\n",
       " '&',\n",
       " 'and',\n",
       " 'is',\n",
       " '!',\n",
       " 'cervical']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = load_dict('model/dict.dd')\n",
    "dictionary[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word to index\n",
    "X_train_f = [[dictionary.index(word) if word in dictionary else -1 for word in doc] for doc in X_train]\n",
    "X_test_f = [[dictionary.index(word) if word in dictionary else -1 for word in doc] for doc in X_test]\n",
    "X_pred_f = [[dictionary.index(word) if word in dictionary else -1 for word in doc] for doc in X_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 0, 1, 417, 0, 1, 17, 5, 730, 88, 7, 13, 6, 353, 105, 7, 96, 15, 77, 9, 6, 10, 3236, 8, 347, 755, 1537], [2, 0, 27, 0, 1, 3, 186, 8, 307, 327, 10, 91, 41], [1892, 2084, 3492, 22, 1, 25, 1012], [57, 24, 682, 7, 5, 238, 245, 73, 16, 311, 326, 21], [1, 28, 208, 201, 164, 503, 5388, 126, 66, 1, 3, 63, 592, 53, 8, 117, 119, 69, 4194]]\n",
      "[[210, 338, 41, 215, 165, 34, 1, 471, 4, 459, 182, 409, 29, 20, 411, 4], [13, 180, 57, 1813, 2238, 296, 5, 1477, 1003, 134, 6, 3875, 616, 8, 448, 67, 4], [2, 0, 27, 0, 14, 262, 7, 346, 362, 10, 14, 509, 110, 118, 250, 34, 168, 1, 3], [726, 12, 8803, 423, 23, 155, 15, 281, 223, 397, 8, 1, 0, 492, 287], [97, 1, 16, 9, 153, 2965, 3326, 3551, 22, 2585, 193, 18, 1077, 125, 8, 159, 0]]\n",
      "[[2, 0, 1, 76, 872, 12, 175, 229, 78, 17, 14, 174, 38, 35, 226, 8, 353, 47, 16, 354, 648, 10, 140, 4], [2, 0, 13, 5664, 30, 396, 81, 844, 484, 158, 1003, 44, 7, 41, 1808, 499, 30, 98], [596, 40, 714, 24, 6139, 182, 151, 48, 16837, 5, 417, 38, 11, 17, 36, 561, 38, 175, 992, 174, 6296, 4], [2, 0, 4, 513, 38, 75, 15, 41, 39, 5, 1, 3, 245, 5, 302, 7, 1423, 24, 35, 39, 5, 3, 1144, 24, 230, 871, 5293], [3, 63, 859, 497, 1, 10, 367, 41, 76, 32, 446, 101, 40, 132, 78, 6, 27, 200, 1, 367, 75, 418, 65, 53, 18]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_f[:5])\n",
    "print(X_test_f[:5])\n",
    "print(X_pred_f[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding words using GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 2, 17.016061533133225)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array([len(doc) for doc in X_train_f])\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 40\n",
    "embedding_dim = 50\n",
    "vecs, words, wordidx = load_vectors('twitter.27B.%dd'%(embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_u = sequence.pad_sequences(X_train_f, maxlen=seq_len)\n",
    "X_test_u = sequence.pad_sequences(X_test_f, maxlen=seq_len)\n",
    "X_pred_u = sequence.pad_sequences(X_pred_f, maxlen=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    2\n",
      "     0    1  417    0    1   17    5  730   88    7   13    6  353  105\n",
      "     7   96   15   77    9    6   10 3236    8  347  755 1537]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    2\n",
      "     0   27    0    1    3  186    8  307  327   10   91   41]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0 1892 2084 3492   22    1   25 1012]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    57   24  682    7    5  238  245   73   16  311  326   21]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    1   28  208  201  164  503 5388\n",
      "   126   66    1    3   63  592   53    8  117  119   69 4194]]\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0  210  338   41  215\n",
      "   165   34    1  471    4  459  182  409   29   20  411    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0   13  180   57 1813 2238\n",
      "   296    5 1477 1003  134    6 3875  616    8  448   67    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    2    0   27    0   14  262    7\n",
      "   346  362   10   14  509  110  118  250   34  168    1    3]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0  726   12 8803\n",
      "   423   23  155   15  281  223  397    8    1    0  492  287]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0   97    1   16    9  153\n",
      "  2965 3326 3551   22 2585  193   18 1077  125    8  159    0]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     2     0     1    76   872    12   175   229\n",
      "     78    17    14   174    38    35   226     8   353    47    16   354\n",
      "    648    10   140     4]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     2     0\n",
      "     13  5664    30   396    81   844   484   158  1003    44     7    41\n",
      "   1808   499    30    98]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0   596    40   714    24  6139   182\n",
      "    151    48 16837     5   417    38    11    17    36   561    38   175\n",
      "    992   174  6296     4]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     2     0     4   513    38    75    15    41    39     5     1\n",
      "      3   245     5   302     7  1423    24    35    39     5     3  1144\n",
      "     24   230   871  5293]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     3    63   859   497     1    10   367    41    76\n",
      "     32   446   101    40   132    78     6    27   200     1   367    75\n",
      "    418    65    53    18]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_u[:5])\n",
    "print(X_test_u[:5])\n",
    "print(X_pred_u[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embedding(dictionary):\n",
    "    print(vecs.shape)\n",
    "    n_fact = vecs.shape[1]\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "\n",
    "    for i, word in enumerate(dictionary):\n",
    "        #if word:# and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
    "        src_idx = wordidx[word] if word in wordidx else 0\n",
    "        \n",
    "        if src_idx:\n",
    "            emb[i] = vecs[src_idx]\n",
    "        else:\n",
    "            # If we can't find the word in glove, randomly initialize\n",
    "            emb[i] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "\n",
    "    # This is our \"rare word\" id - we want to randomly initialize\n",
    "    emb[-1] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "    emb/=3\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<user>': 0,\n",
       " '.': 1,\n",
       " ':': 2,\n",
       " 'rt': 3,\n",
       " ',': 4,\n",
       " '<repeat>': 5,\n",
       " '<hashtag>': 6,\n",
       " '<number>': 7,\n",
       " '<url>': 8,\n",
       " '!': 9,\n",
       " 'i': 10,\n",
       " 'a': 11,\n",
       " '\"': 12,\n",
       " 'the': 13,\n",
       " '?': 14,\n",
       " 'you': 15,\n",
       " 'to': 16,\n",
       " '(': 17,\n",
       " '<allcaps>': 18,\n",
       " '<elong>': 19,\n",
       " ')': 20,\n",
       " 'me': 21,\n",
       " 'de': 22,\n",
       " '<smile>': 23,\n",
       " '！': 24,\n",
       " 'que': 25,\n",
       " 'and': 26,\n",
       " '。': 27,\n",
       " '-': 28,\n",
       " 'my': 29,\n",
       " 'no': 30,\n",
       " '、': 31,\n",
       " 'is': 32,\n",
       " 'it': 33,\n",
       " '…': 34,\n",
       " 'in': 35,\n",
       " 'n': 36,\n",
       " 'for': 37,\n",
       " '/': 38,\n",
       " 'of': 39,\n",
       " 'la': 40,\n",
       " \"'s\": 41,\n",
       " '*': 42,\n",
       " 'do': 43,\n",
       " \"n't\": 44,\n",
       " 'that': 45,\n",
       " 'on': 46,\n",
       " 'y': 47,\n",
       " \"'\": 48,\n",
       " 'e': 49,\n",
       " 'o': 50,\n",
       " 'u': 51,\n",
       " 'en': 52,\n",
       " 'this': 53,\n",
       " 'el': 54,\n",
       " 'so': 55,\n",
       " 'be': 56,\n",
       " \"'m\": 57,\n",
       " 'with': 58,\n",
       " 'just': 59,\n",
       " '>': 60,\n",
       " 'your': 61,\n",
       " '^': 62,\n",
       " 'like': 63,\n",
       " 'have': 64,\n",
       " 'te': 65,\n",
       " 'at': 66,\n",
       " '？': 67,\n",
       " 'love': 68,\n",
       " 'se': 69,\n",
       " 'are': 70,\n",
       " '<': 71,\n",
       " 'm': 72,\n",
       " 'r': 73,\n",
       " 'if': 74,\n",
       " 'all': 75,\n",
       " 'b': 76,\n",
       " '・': 77,\n",
       " 'not': 78,\n",
       " 'but': 79,\n",
       " 'we': 80,\n",
       " 'es': 81,\n",
       " 'ya': 82,\n",
       " '&': 83,\n",
       " 'follow': 84,\n",
       " 'up': 85,\n",
       " 'what': 86,\n",
       " 'get': 87,\n",
       " 'lol': 88,\n",
       " 'un': 89,\n",
       " '♥': 90,\n",
       " 'lo': 91,\n",
       " 'when': 92,\n",
       " 'was': 93,\n",
       " '“': 94,\n",
       " '”': 95,\n",
       " 'one': 96,\n",
       " 'por': 97,\n",
       " 'si': 98,\n",
       " 'out': 99,\n",
       " '_': 100,\n",
       " 'mi': 101,\n",
       " 'can': 102,\n",
       " '<sadface>': 103,\n",
       " 'من': 104,\n",
       " '♡': 105,\n",
       " '´': 106,\n",
       " 'he': 107,\n",
       " 'con': 108,\n",
       " 'they': 109,\n",
       " 'now': 110,\n",
       " 'go': 111,\n",
       " '،': 112,\n",
       " 'para': 113,\n",
       " 'los': 114,\n",
       " 'know': 115,\n",
       " 'haha': 116,\n",
       " 'good': 117,\n",
       " 'tu': 118,\n",
       " 'back': 119,\n",
       " '~': 120,\n",
       " 'about': 121,\n",
       " 'new': 122,\n",
       " ';': 123,\n",
       " 'as': 124,\n",
       " 'day': 125,\n",
       " 'how': 126,\n",
       " 'who': 127,\n",
       " 'will': 128,\n",
       " 'want': 129,\n",
       " 'people': 130,\n",
       " 'yo': 131,\n",
       " 'eu': 132,\n",
       " 'from': 133,\n",
       " 'di': 134,\n",
       " 'time': 135,\n",
       " '<heart>': 136,\n",
       " 's': 137,\n",
       " 'aku': 138,\n",
       " 'da': 139,\n",
       " \"'re\": 140,\n",
       " '<lolface>': 141,\n",
       " 'una': 142,\n",
       " 'got': 143,\n",
       " 'las': 144,\n",
       " 'more': 145,\n",
       " 'x': 146,\n",
       " 'she': 147,\n",
       " 'today': 148,\n",
       " '（': 149,\n",
       " '>>': 150,\n",
       " 'k': 151,\n",
       " 'by': 152,\n",
       " 'or': 153,\n",
       " 'في': 154,\n",
       " '･': 155,\n",
       " 'too': 156,\n",
       " 'le': 157,\n",
       " 'é': 158,\n",
       " '|': 159,\n",
       " '[': 160,\n",
       " '）': 161,\n",
       " ']': 162,\n",
       " 'see': 163,\n",
       " 'why': 164,\n",
       " 'yg': 165,\n",
       " 'ca': 166,\n",
       " 'como': 167,\n",
       " 'her': 168,\n",
       " '—': 169,\n",
       " 'q': 170,\n",
       " 'need': 171,\n",
       " 'an': 172,\n",
       " 'na': 173,\n",
       " '笑': 174,\n",
       " 'there': 175,\n",
       " 'ω': 176,\n",
       " 'happy': 177,\n",
       " 'im': 178,\n",
       " 'mas': 179,\n",
       " 'je': 180,\n",
       " 'life': 181,\n",
       " 'really': 182,\n",
       " 'make': 183,\n",
       " 'yang': 184,\n",
       " 'shit': 185,\n",
       " 'think': 186,\n",
       " 't': 187,\n",
       " '❤': 188,\n",
       " 'não': 189,\n",
       " 'never': 190,\n",
       " 'some': 191,\n",
       " '～': 192,\n",
       " 'oh': 193,\n",
       " '★': 194,\n",
       " 'did': 195,\n",
       " 'would': 196,\n",
       " 'del': 197,\n",
       " '`': 198,\n",
       " 'd': 199,\n",
       " 'please': 200,\n",
       " 'via': 201,\n",
       " 'much': 202,\n",
       " 'fuck': 203,\n",
       " 'al': 204,\n",
       " 'dia': 205,\n",
       " '$': 206,\n",
       " 'و': 207,\n",
       " 'right': 208,\n",
       " 'best': 209,\n",
       " 'c': 210,\n",
       " 'going': 211,\n",
       " 'الله': 212,\n",
       " 'pero': 213,\n",
       " 'only': 214,\n",
       " 'has': 215,\n",
       " '♪': 216,\n",
       " \"'ll\": 217,\n",
       " 'twitter': 218,\n",
       " '=': 219,\n",
       " 'hahaha': 220,\n",
       " 'its': 221,\n",
       " 'nn': 222,\n",
       " '｀': 223,\n",
       " '¿': 224,\n",
       " 'am': 225,\n",
       " 'say': 226,\n",
       " '<neutralface>': 227,\n",
       " 'them': 228,\n",
       " 'here': 229,\n",
       " 'لا': 230,\n",
       " 'off': 231,\n",
       " 'still': 232,\n",
       " 'dan': 233,\n",
       " '+': 234,\n",
       " 'night': 235,\n",
       " 'w': 236,\n",
       " 'ada': 237,\n",
       " 'someone': 238,\n",
       " 'even': 239,\n",
       " 'then': 240,\n",
       " '☆': 241,\n",
       " 'ni': 242,\n",
       " 'come': 243,\n",
       " 'com': 244,\n",
       " 'always': 245,\n",
       " 'man': 246,\n",
       " \"'ve\": 247,\n",
       " 'been': 248,\n",
       " 'his': 249,\n",
       " 'itu': 250,\n",
       " 'على': 251,\n",
       " '-_-': 252,\n",
       " '☺': 253,\n",
       " 'over': 254,\n",
       " 'um': 255,\n",
       " 'ما': 256,\n",
       " 'hate': 257,\n",
       " 'girl': 258,\n",
       " 'ai': 259,\n",
       " 'had': 260,\n",
       " 'pra': 261,\n",
       " 'todo': 262,\n",
       " 'mais': 263,\n",
       " 'feel': 264,\n",
       " 'let': 265,\n",
       " 'ini': 266,\n",
       " 'because': 267,\n",
       " 'ﾟ': 268,\n",
       " 'thanks': 269,\n",
       " 'ah': 270,\n",
       " 'way': 271,\n",
       " 'ever': 272,\n",
       " 'look': 273,\n",
       " 'tweet': 274,\n",
       " 'followers': 275,\n",
       " 'should': 276,\n",
       " 'our': 277,\n",
       " 'xd': 278,\n",
       " 'aja': 279,\n",
       " 'esta': 280,\n",
       " 'school': 281,\n",
       " 'him': 282,\n",
       " 'ser': 283,\n",
       " 'take': 284,\n",
       " 'than': 285,\n",
       " 'video': 286,\n",
       " 'em': 287,\n",
       " 'last': 288,\n",
       " 'wanna': 289,\n",
       " 'does': 290,\n",
       " 'us': 291,\n",
       " 'miss': 292,\n",
       " 'l': 293,\n",
       " 'ga': 294,\n",
       " 'better': 295,\n",
       " 'well': 296,\n",
       " 'could': 297,\n",
       " '▽': 298,\n",
       " '%': 299,\n",
       " 'apa': 300,\n",
       " 'cuando': 301,\n",
       " 'team': 302,\n",
       " '✔': 303,\n",
       " '@': 304,\n",
       " 'ok': 305,\n",
       " '؟': 306,\n",
       " '•': 307,\n",
       " 'vida': 308,\n",
       " 'quiero': 309,\n",
       " 'les': 310,\n",
       " 'being': 311,\n",
       " 'real': 312,\n",
       " 'down': 313,\n",
       " 'kamu': 314,\n",
       " 'everyone': 315,\n",
       " 'gonna': 316,\n",
       " 'live': 317,\n",
       " 'tonight': 318,\n",
       " 'yes': 319,\n",
       " 'work': 320,\n",
       " 'ass': 321,\n",
       " 'retweet': 322,\n",
       " 'nada': 323,\n",
       " 'sama': 324,\n",
       " 'first': 325,\n",
       " '<<': 326,\n",
       " 'photo': 327,\n",
       " 'tomorrow': 328,\n",
       " 'where': 329,\n",
       " 'god': 330,\n",
       " 'son': 331,\n",
       " 'ke': 332,\n",
       " 'ta': 333,\n",
       " 'f': 334,\n",
       " 'home': 335,\n",
       " 'lagi': 336,\n",
       " 'thank': 337,\n",
       " 'birthday': 338,\n",
       " '█': 339,\n",
       " 'ha': 340,\n",
       " 'great': 341,\n",
       " 'lmao': 342,\n",
       " 'omg': 343,\n",
       " 'morning': 344,\n",
       " 'más': 345,\n",
       " 'mau': 346,\n",
       " 'baby': 347,\n",
       " 'dont': 348,\n",
       " '｡': 349,\n",
       " 'their': 350,\n",
       " 'p': 351,\n",
       " 'things': 352,\n",
       " 'game': 353,\n",
       " 'pas': 354,\n",
       " 'bad': 355,\n",
       " 'year': 356,\n",
       " 'yeah': 357,\n",
       " 'su': 358,\n",
       " 'bitch': 359,\n",
       " 'в': 360,\n",
       " 'stop': 361,\n",
       " 'hoy': 362,\n",
       " 'something': 363,\n",
       " 'meu': 364,\n",
       " 'tak': 365,\n",
       " 'gak': 366,\n",
       " 'world': 367,\n",
       " 'amor': 368,\n",
       " 'h': 369,\n",
       " '\\\\': 370,\n",
       " 'ver': 371,\n",
       " '；': 372,\n",
       " 'porque': 373,\n",
       " 'give': 374,\n",
       " 'these': 375,\n",
       " 'اللهم': 376,\n",
       " 'were': 377,\n",
       " 'hay': 378,\n",
       " 'sleep': 379,\n",
       " 'gue': 380,\n",
       " 'every': 381,\n",
       " 'friends': 382,\n",
       " 'uma': 383,\n",
       " 'tell': 384,\n",
       " 'amo': 385,\n",
       " 'vou': 386,\n",
       " 'bien': 387,\n",
       " '¡': 388,\n",
       " 'again': 389,\n",
       " '＾': 390,\n",
       " '／': 391,\n",
       " 'done': 392,\n",
       " 'after': 393,\n",
       " 'todos': 394,\n",
       " 'girls': 395,\n",
       " 'guys': 396,\n",
       " 'getting': 397,\n",
       " 'big': 398,\n",
       " 'wait': 399,\n",
       " 'justin': 400,\n",
       " 'eh': 401,\n",
       " '→': 402,\n",
       " 'kan': 403,\n",
       " 'kita': 404,\n",
       " 'jajaja': 405,\n",
       " 'wish': 406,\n",
       " 'said': 407,\n",
       " 'fucking': 408,\n",
       " 'show': 409,\n",
       " 'thing': 410,\n",
       " 'next': 411,\n",
       " 'você': 412,\n",
       " 'nos': 413,\n",
       " 'little': 414,\n",
       " 'tengo': 415,\n",
       " 'keep': 416,\n",
       " 'person': 417,\n",
       " \"''\": 418,\n",
       " '∀': 419,\n",
       " 'hope': 420,\n",
       " 'كل': 421,\n",
       " 'hey': 422,\n",
       " 'bisa': 423,\n",
       " 'free': 424,\n",
       " 'made': 425,\n",
       " 'foto': 426,\n",
       " 'va': 427,\n",
       " 'everything': 428,\n",
       " 'iya': 429,\n",
       " 'nigga': 430,\n",
       " 'eso': 431,\n",
       " 'et': 432,\n",
       " 'watch': 433,\n",
       " 'music': 434,\n",
       " 'week': 435,\n",
       " 'talk': 436,\n",
       " 'ne': 437,\n",
       " 'solo': 438,\n",
       " 'gente': 439,\n",
       " 'udah': 440,\n",
       " '：': 441,\n",
       " '--': 442,\n",
       " '＼': 443,\n",
       " 'mejor': 444,\n",
       " 'facebook': 445,\n",
       " 'ma': 446,\n",
       " 'v': 447,\n",
       " 'phone': 448,\n",
       " 'most': 449,\n",
       " 'same': 450,\n",
       " 'okay': 451,\n",
       " 'ik': 452,\n",
       " 'before': 453,\n",
       " 'minha': 454,\n",
       " 'days': 455,\n",
       " 'g': 456,\n",
       " 'ti': 457,\n",
       " 'damn': 458,\n",
       " 'nice': 459,\n",
       " 'voy': 460,\n",
       " 'vai': 461,\n",
       " 'call': 462,\n",
       " 'long': 463,\n",
       " 'tapi': 464,\n",
       " 'http': 465,\n",
       " 'sin': 466,\n",
       " 'nunca': 467,\n",
       " 'doing': 468,\n",
       " 'other': 469,\n",
       " 'find': 470,\n",
       " 'il': 471,\n",
       " 'sa': 472,\n",
       " 'sorry': 473,\n",
       " 'nya': 474,\n",
       " 'orang': 475,\n",
       " '°': 476,\n",
       " 'hard': 477,\n",
       " 'mean': 478,\n",
       " 'die': 479,\n",
       " 'اللي': 480,\n",
       " 'tem': 481,\n",
       " 'soy': 482,\n",
       " 'este': 483,\n",
       " 'kalo': 484,\n",
       " 'só': 485,\n",
       " 'th': 486,\n",
       " 'win': 487,\n",
       " 'nothing': 488,\n",
       " 'into': 489,\n",
       " 'face': 490,\n",
       " 'cute': 491,\n",
       " \"'d\": 492,\n",
       " 'gracias': 493,\n",
       " 'lah': 494,\n",
       " 'и': 495,\n",
       " 'any': 496,\n",
       " 'play': 497,\n",
       " '←': 498,\n",
       " 'ko': 499,\n",
       " 'text': 500,\n",
       " '⌣': 501,\n",
       " 'estoy': 502,\n",
       " 'tau': 503,\n",
       " 'ur': 504,\n",
       " 'buat': 505,\n",
       " '#': 506,\n",
       " 'cause': 507,\n",
       " 'я': 508,\n",
       " 'put': 509,\n",
       " 'kau': 510,\n",
       " 'siempre': 511,\n",
       " 'juga': 512,\n",
       " 'casa': 513,\n",
       " 'أن': 514,\n",
       " 'help': 515,\n",
       " 'start': 516,\n",
       " 'feliz': 517,\n",
       " 'old': 518,\n",
       " 'ir': 519,\n",
       " 'very': 520,\n",
       " 'care': 521,\n",
       " 'bir': 522,\n",
       " 'makes': 523,\n",
       " 'song': 524,\n",
       " 'check': 525,\n",
       " 'watching': 526,\n",
       " 'ahora': 527,\n",
       " 'jadi': 528,\n",
       " 'os': 529,\n",
       " 'may': 530,\n",
       " 'friend': 531,\n",
       " 'beautiful': 532,\n",
       " 'heart': 533,\n",
       " 'ka': 534,\n",
       " 'vc': 535,\n",
       " 'mundo': 536,\n",
       " 'на': 537,\n",
       " 'sure': 538,\n",
       " 'tan': 539,\n",
       " 'pretty': 540,\n",
       " 'aqui': 541,\n",
       " 'не': 542,\n",
       " 'house': 543,\n",
       " 'رتويت': 544,\n",
       " 'يا': 545,\n",
       " 'ja': 546,\n",
       " 'true': 547,\n",
       " 'muy': 548,\n",
       " 'away': 549,\n",
       " 'already': 550,\n",
       " 'actually': 551,\n",
       " 'believe': 552,\n",
       " 'try': 553,\n",
       " 'many': 554,\n",
       " 'mañana': 555,\n",
       " 'mis': 556,\n",
       " 'lu': 557,\n",
       " 'those': 558,\n",
       " 'hot': 559,\n",
       " 'qué': 560,\n",
       " 'mal': 561,\n",
       " 'عن': 562,\n",
       " 'though': 563,\n",
       " 'ask': 564,\n",
       " 'amazing': 565,\n",
       " 'bed': 566,\n",
       " '}': 567,\n",
       " 'two': 568,\n",
       " 'mom': 569,\n",
       " 'día': 570,\n",
       " 've': 571,\n",
       " 'dari': 572,\n",
       " 'gameinsight': 573,\n",
       " 'stay': 574,\n",
       " 'fun': 575,\n",
       " 'around': 576,\n",
       " 'van': 577,\n",
       " 'cont': 578,\n",
       " 'ready': 579,\n",
       " 'money': 580,\n",
       " 'bu': 581,\n",
       " 'funny': 582,\n",
       " 'cool': 583,\n",
       " 'hair': 584,\n",
       " 'à': 585,\n",
       " 'tho': 586,\n",
       " '{': 587,\n",
       " 'wo': 588,\n",
       " 'hi': 589,\n",
       " 'name': 590,\n",
       " 'tiene': 591,\n",
       " 'hahahaha': 592,\n",
       " 'pa': 593,\n",
       " 'algo': 594,\n",
       " 'gotta': 595,\n",
       " 'ولا': 596,\n",
       " 'boy': 597,\n",
       " 'another': 598,\n",
       " \"c'est\": 599,\n",
       " 'hari': 600,\n",
       " 'jajajaja': 601,\n",
       " 'having': 602,\n",
       " 'cara': 603,\n",
       " 'jaja': 604,\n",
       " 'dm': 605,\n",
       " 'looking': 606,\n",
       " 'top': 607,\n",
       " 'android': 608,\n",
       " 'dah': 609,\n",
       " 'wow': 610,\n",
       " '░': 611,\n",
       " 'eres': 612,\n",
       " 'ben': 613,\n",
       " 'must': 614,\n",
       " 'news': 615,\n",
       " 'met': 616,\n",
       " 'está': 617,\n",
       " 'nih': 618,\n",
       " 'family': 619,\n",
       " 'black': 620,\n",
       " 'thought': 621,\n",
       " 'nak': 622,\n",
       " 'super': 623,\n",
       " 'end': 624,\n",
       " 'hace': 625,\n",
       " 'remember': 626,\n",
       " 'ama': 627,\n",
       " 'party': 628,\n",
       " 'cant': 629,\n",
       " 'vamos': 630,\n",
       " 'anything': 631,\n",
       " 'anyone': 632,\n",
       " 'فولو': 633,\n",
       " 'perfect': 634,\n",
       " 'guy': 635,\n",
       " 'vez': 636,\n",
       " 'christmas': 637,\n",
       " 'dos': 638,\n",
       " 'bueno': 639,\n",
       " 'nao': 640,\n",
       " 'years': 641,\n",
       " 'vote': 642,\n",
       " 'dormir': 643,\n",
       " 'bro': 644,\n",
       " 'else': 645,\n",
       " 'quien': 646,\n",
       " 'untuk': 647,\n",
       " 'jangan': 648,\n",
       " 'myself': 649,\n",
       " 'head': 650,\n",
       " 'mind': 651,\n",
       " 'gua': 652,\n",
       " 'talking': 653,\n",
       " 'while': 654,\n",
       " 'dat': 655,\n",
       " 'food': 656,\n",
       " 'д': 657,\n",
       " 'coming': 658,\n",
       " 'wkwk': 659,\n",
       " 'trying': 660,\n",
       " 'saya': 661,\n",
       " 'mucho': 662,\n",
       " 'without': 663,\n",
       " 'wrong': 664,\n",
       " '’s': 665,\n",
       " 'baru': 666,\n",
       " '__': 667,\n",
       " 'hehe': 668,\n",
       " 'hacer': 669,\n",
       " 'lot': 670,\n",
       " 'followed': 671,\n",
       " 'crazy': 672,\n",
       " 'hell': 673,\n",
       " 'feeling': 674,\n",
       " 'des': 675,\n",
       " 'kok': 676,\n",
       " 'j': 677,\n",
       " 'stats': 678,\n",
       " \"j'\": 679,\n",
       " 'ان': 680,\n",
       " 'tweets': 681,\n",
       " 'non': 682,\n",
       " 'cosas': 683,\n",
       " 'era': 684,\n",
       " 'high': 685,\n",
       " 'niggas': 686,\n",
       " 'change': 687,\n",
       " 'movie': 688,\n",
       " 'xx': 689,\n",
       " 'mad': 690,\n",
       " 'sih': 691,\n",
       " 'sometimes': 692,\n",
       " 'deh': 693,\n",
       " 'allah': 694,\n",
       " 'through': 695,\n",
       " 'pour': 696,\n",
       " 'ela': 697,\n",
       " 'soon': 698,\n",
       " 'gone': 699,\n",
       " 'playing': 700,\n",
       " 'smile': 701,\n",
       " 'bukan': 702,\n",
       " 'tv': 703,\n",
       " 'fans': 704,\n",
       " 'hasta': 705,\n",
       " 'akan': 706,\n",
       " \"y'\": 707,\n",
       " 'looks': 708,\n",
       " 'isso': 709,\n",
       " '✌': 710,\n",
       " 'tired': 711,\n",
       " 'boys': 712,\n",
       " 'might': 713,\n",
       " 'dong': 714,\n",
       " 'lg': 715,\n",
       " 'use': 716,\n",
       " 'maybe': 717,\n",
       " 'until': 718,\n",
       " 'menos': 719,\n",
       " 'own': 720,\n",
       " 'dengan': 721,\n",
       " 'eat': 722,\n",
       " 'ou': 723,\n",
       " 'weekend': 724,\n",
       " '˘': 725,\n",
       " 'class': 726,\n",
       " 'ele': 727,\n",
       " 'harry': 728,\n",
       " 'iphone': 729,\n",
       " 'friday': 730,\n",
       " 'single': 731,\n",
       " 'ff': 732,\n",
       " 'awesome': 733,\n",
       " 'bout': 734,\n",
       " 'muito': 735,\n",
       " 'hoje': 736,\n",
       " '¬': 737,\n",
       " 'dios': 738,\n",
       " 'such': 739,\n",
       " 'estar': 740,\n",
       " 'já': 741,\n",
       " 'quando': 742,\n",
       " 'esa': 743,\n",
       " 'making': 744,\n",
       " '━': 745,\n",
       " 'times': 746,\n",
       " 'lmfao': 747,\n",
       " 'gw': 748,\n",
       " 'moment': 749,\n",
       " 'yet': 750,\n",
       " 'aw': 751,\n",
       " 'smh': 752,\n",
       " 'banget': 753,\n",
       " 'masih': 754,\n",
       " 'qui': 755,\n",
       " 'quem': 756,\n",
       " '–': 757,\n",
       " 'leave': 758,\n",
       " 'du': 759,\n",
       " 'une': 760,\n",
       " 'guess': 761,\n",
       " 'hit': 762,\n",
       " 'с': 763,\n",
       " 'pm': 764,\n",
       " 'since': 765,\n",
       " 'pues': 766,\n",
       " 'est': 767,\n",
       " 'job': 768,\n",
       " 'ﾉ': 769,\n",
       " 'mana': 770,\n",
       " 'bom': 771,\n",
       " 'siapa': 772,\n",
       " 'suka': 773,\n",
       " 'bieber': 774,\n",
       " 'mention': 775,\n",
       " 'lebih': 776,\n",
       " 'favorite': 777,\n",
       " 'bitches': 778,\n",
       " 'forever': 779,\n",
       " 'لي': 780,\n",
       " 'final': 781,\n",
       " 'read': 782,\n",
       " 'alguien': 783,\n",
       " 'open': 784,\n",
       " 'yourself': 785,\n",
       " 'ese': 786,\n",
       " 'che': 787,\n",
       " 'sex': 788,\n",
       " 'yaa': 789,\n",
       " 'car': 790,\n",
       " 'direction': 791,\n",
       " 'tidak': 792,\n",
       " 'seu': 793,\n",
       " 'gets': 794,\n",
       " 'left': 795,\n",
       " 're': 796,\n",
       " 'jam': 797,\n",
       " 'enough': 798,\n",
       " 'إلا': 799,\n",
       " 'once': 800,\n",
       " '’': 801,\n",
       " 'part': 802,\n",
       " 'cada': 803,\n",
       " '定期': 804,\n",
       " 'لك': 805,\n",
       " 'een': 806,\n",
       " 'seen': 807,\n",
       " 'kak': 808,\n",
       " 'así': 809,\n",
       " 'nem': 810,\n",
       " 'عمل': 811,\n",
       " 'white': 812,\n",
       " 'told': 813,\n",
       " 'says': 814,\n",
       " 'esto': 815,\n",
       " 'sad': 816,\n",
       " 'mo': 817,\n",
       " 'fue': 818,\n",
       " 'yah': 819,\n",
       " 'summer': 820,\n",
       " 'ه': 821,\n",
       " '⭕': 822,\n",
       " '»': 823,\n",
       " 'thats': 824,\n",
       " 'مع': 825,\n",
       " 'posted': 826,\n",
       " 'wants': 827,\n",
       " 'agora': 828,\n",
       " 'together': 829,\n",
       " 'fan': 830,\n",
       " 'men': 831,\n",
       " 'hear': 832,\n",
       " 'full': 833,\n",
       " '☀': 834,\n",
       " 'sigo': 835,\n",
       " 'pq': 836,\n",
       " 'dulu': 837,\n",
       " 'plus': 838,\n",
       " 'foi': 839,\n",
       " 'tudo': 840,\n",
       " 'هو': 841,\n",
       " 'ill': 842,\n",
       " 'あ': 843,\n",
       " 'thinking': 844,\n",
       " 'wtf': 845,\n",
       " 'pagi': 846,\n",
       " 'mama': 847,\n",
       " 'kalau': 848,\n",
       " 'hati': 849,\n",
       " 'sexy': 850,\n",
       " 'sayang': 851,\n",
       " 'baik': 852,\n",
       " 'semua': 853,\n",
       " 'hola': 854,\n",
       " 'went': 855,\n",
       " 'vos': 856,\n",
       " 'tanto': 857,\n",
       " 'finally': 858,\n",
       " 'fb': 859,\n",
       " 'sea': 860,\n",
       " 'stupid': 861,\n",
       " 'tus': 862,\n",
       " 'seriously': 863,\n",
       " 'hora': 864,\n",
       " 'min': 865,\n",
       " 'pic': 866,\n",
       " 'estas': 867,\n",
       " 'turn': 868,\n",
       " 'hours': 869,\n",
       " 'excited': 870,\n",
       " 'nah': 871,\n",
       " 'buy': 872,\n",
       " 'saying': 873,\n",
       " 'mah': 874,\n",
       " 'break': 875,\n",
       " 'needs': 876,\n",
       " 'ce': 877,\n",
       " 'room': 878,\n",
       " 'choice': 879,\n",
       " 'far': 880,\n",
       " 'dead': 881,\n",
       " 'quero': 882,\n",
       " 'saw': 883,\n",
       " 'kids': 884,\n",
       " 'lil': 885,\n",
       " 'whole': 886,\n",
       " 'puede': 887,\n",
       " 'fall': 888,\n",
       " 'sus': 889,\n",
       " 'lost': 890,\n",
       " 'asi': 891,\n",
       " 'word': 892,\n",
       " '☹': 893,\n",
       " 'also': 894,\n",
       " 'ريتويت': 895,\n",
       " 'probably': 896,\n",
       " 'everybody': 897,\n",
       " 'tarde': 898,\n",
       " 'run': 899,\n",
       " 'sei': 900,\n",
       " 'follback': 901,\n",
       " 'forget': 902,\n",
       " 'sweet': 903,\n",
       " 'welcome': 904,\n",
       " 'selamat': 905,\n",
       " '＿': 906,\n",
       " 'sur': 907,\n",
       " 'place': 908,\n",
       " 'gusta': 909,\n",
       " 'sabe': 910,\n",
       " 'androidgames': 911,\n",
       " 'tp': 912,\n",
       " 'tiempo': 913,\n",
       " 'بس': 914,\n",
       " 'sou': 915,\n",
       " 'tuh': 916,\n",
       " 'vs': 917,\n",
       " 'eyes': 918,\n",
       " 'انا': 919,\n",
       " 'picture': 920,\n",
       " 'das': 921,\n",
       " 'meet': 922,\n",
       " 'anak': 923,\n",
       " 'persona': 924,\n",
       " 'essa': 925,\n",
       " 'bored': 926,\n",
       " 'following': 927,\n",
       " 'nadie': 928,\n",
       " 'nobody': 929,\n",
       " 'dice': 930,\n",
       " 'alone': 931,\n",
       " 'sick': 932,\n",
       " 'red': 933,\n",
       " 'city': 934,\n",
       " 'cinta': 935,\n",
       " '月': 936,\n",
       " 'linda': 937,\n",
       " 'dream': 938,\n",
       " 'story': 939,\n",
       " 'km': 940,\n",
       " 'het': 941,\n",
       " 'waiting': 942,\n",
       " '^_^': 943,\n",
       " 'mine': 944,\n",
       " 'что': 945,\n",
       " 'reason': 946,\n",
       " 'kk': 947,\n",
       " 'لو': 948,\n",
       " 'online': 949,\n",
       " 'fast': 950,\n",
       " 'udh': 951,\n",
       " 'wanted': 952,\n",
       " 'op': 953,\n",
       " 'others': 954,\n",
       " 'gay': 955,\n",
       " 'n’t': 956,\n",
       " 'used': 957,\n",
       " 'sem': 958,\n",
       " 'understand': 959,\n",
       " 'moi': 960,\n",
       " 'sm': 961,\n",
       " 'aint': 962,\n",
       " 'donde': 963,\n",
       " 'bem': 964,\n",
       " 'which': 965,\n",
       " 'ng': 966,\n",
       " 'followback': 967,\n",
       " 'punya': 968,\n",
       " 'late': 969,\n",
       " 'anda': 970,\n",
       " 'tidur': 971,\n",
       " 'puedo': 972,\n",
       " 'early': 973,\n",
       " 'nd': 974,\n",
       " 'personas': 975,\n",
       " 'banyak': 976,\n",
       " '✅': 977,\n",
       " '➊': 978,\n",
       " 'trust': 979,\n",
       " 'noche': 980,\n",
       " 'tl': 981,\n",
       " '＞': 982,\n",
       " '«': 983,\n",
       " 'af': 984,\n",
       " 'move': 985,\n",
       " 'pro': 986,\n",
       " 'bring': 987,\n",
       " 'ku': 988,\n",
       " 'called': 989,\n",
       " 'relationship': 990,\n",
       " 'idk': 991,\n",
       " 'hurt': 992,\n",
       " 'st': 993,\n",
       " 'pernah': 994,\n",
       " 'pessoas': 995,\n",
       " 'hello': 996,\n",
       " 'uno': 997,\n",
       " 'unfollowers': 998,\n",
       " 'cry': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1193517, 50)\n"
     ]
    }
   ],
   "source": [
    "embedding = create_embedding(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create CNN mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\conda\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 50)            1165350   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 40, 50)            12550     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 50)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40, 11)            561       \n",
      "=================================================================\n",
      "Total params: 1,178,461\n",
      "Trainable params: 13,111\n",
      "Non-trainable params: 1,165,350\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dropout_threshold = 0.2\n",
    "model = Sequential()\n",
    "\n",
    "#embedding layer\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=seq_len, weights=[embedding], trainable=False, dropout=0.2))\n",
    "# model.add(Dropout(dropout_threshold))\n",
    "model.add(Conv1D(embedding_dim, 5, padding='same', activation='relu'))\n",
    "model.add(Dropout(dropout_threshold))\n",
    "model.add(Dense(11, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "#also can use SDG as optimizer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_1 to have 3 dimensions, but got array with shape (69296, 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-4e9b1f331b3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_u\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_u\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\conda\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mE:\\conda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1520\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1521\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1522\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1523\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1524\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\conda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1380\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1383\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1384\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32mE:\\conda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    130\u001b[0m                                  \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m                                  \u001b[1;34m' dimensions, but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                                  str(array.shape))\n\u001b[0m\u001b[0;32m    133\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_1 to have 3 dimensions, but got array with shape (69296, 11)"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable=True\n",
    "model.fit(X_train_u, y_train, validation_data=(X_test_u, y_test), epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
