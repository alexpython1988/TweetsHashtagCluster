{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets classification by its hashtags and no labeled tweets hashtag predication\n",
    "\n",
    "\n",
    "**Using top n hashtags as label to build a supervised model for tweets classification and hashtag predication**\n",
    "\n",
    "[1.1 load data](1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='1.1'> load packages and modeling data </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4736411848066457814\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow backend\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n",
      "tensorflow\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import *\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "print(keras.__version__)\n",
    "print(keras.backend.backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bcolz\n",
    "import pickle\n",
    "\n",
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "    \n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]\n",
    "\n",
    "def save_dict(fname, dictionary):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(dictionary, f)\n",
    "\n",
    "def load_dict(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hashtag_label': [1, 5],\n",
       " 'hashtags': ['hpv', 'vaccin'],\n",
       " 'id': '418263863772327936',\n",
       " 'orignal_hashtags': ['#hpv', '#vaccine'],\n",
       " 'raw': 'rt @cdcstd: #hpv vax coverage could be 93% if doctors gave hpv #vaccine each time a preteen/teen got any other vaccine&gt; http://t.co/xxryga5â€¦',\n",
       " 'text': 'rt : hpv vax coverage could be 93% if doctors gave hpv vaccine each time a preteen / teen got any other vaccine>',\n",
       " 'words': ['rt',\n",
       "  ':',\n",
       "  'hpv',\n",
       "  'vax',\n",
       "  'coverage',\n",
       "  'could',\n",
       "  'be',\n",
       "  '93',\n",
       "  '%',\n",
       "  'if',\n",
       "  'doctors',\n",
       "  'gave',\n",
       "  'hpv',\n",
       "  'vaccine',\n",
       "  'each',\n",
       "  'time',\n",
       "  'a',\n",
       "  'preteen',\n",
       "  '/',\n",
       "  'teen',\n",
       "  'got',\n",
       "  'any',\n",
       "  'other',\n",
       "  'vaccine',\n",
       "  '>']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load tweets data\n",
    "import json\n",
    "tweets_file = \"temp/tweets4classification.json\"\n",
    "with open(tweets_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    modeling_data = json.load(f)\n",
    "modeling_data['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load processed word enbeddings\n",
    "path = 'wordsenbeddings/'\n",
    "res_path = path + 'results/'\n",
    "\n",
    "def load_vectors(name):\n",
    "    loc = res_path + name\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_glove(name, dim):\n",
    "    with open(path+ 'glove.' + name + '.txt', 'r', encoding=\"utf-8\") as f:\n",
    "        vecs = []\n",
    "        words = []\n",
    "        \n",
    "        for i, line in enumerate(f):\n",
    "            d = line.split()\n",
    "            word = d[0]\n",
    "            vec = np.array(d[1:], dtype=np.float32)\n",
    "            if (len(d) == dim): # this is space\n",
    "                word = ' '\n",
    "                vec = np.array(d, dtype=np.float32)\n",
    "            \n",
    "            words.append(word)            \n",
    "            vecs.append(vec)\n",
    "\n",
    "        wordidx = {o:i for i,o in enumerate(words)}\n",
    "        save_array(res_path+name+'.dat', vecs)\n",
    "        pickle.dump(words, open(res_path+name+'_words.pkl','wb'))\n",
    "        pickle.dump(wordidx, open(res_path+name+'_idx.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_glove('twitter.27B.200d', 200)\n",
    "get_glove('twitter.27B.25d', 25)\n",
    "get_glove('twitter.27B.50d', 50)\n",
    "get_glove('twitter.27B.100d', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare train and test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ list(['rt', ':', 'hpv', 'vax', 'coverage', 'could', 'be', '93', '%', 'if', 'doctors', 'gave', 'hpv', 'vaccine', 'each', 'time', 'a', 'preteen', '/', 'teen', 'got', 'any', 'other', 'vaccine', '>'])\n",
      " list(['rt', ':', 'hpv', 'vax', 'coverage', 'could', 'be', '93', '%', 'if', 'doctors', 'gave', 'hpv', 'vaccine', 'each', 'time', 'a', 'preteen', '/', 'teen', 'got', 'any', 'other', 'vaccine', '...', '.'])]\n",
      "[list([1, 5]) list([1, 5])]\n",
      "81049\n",
      "81049\n"
     ]
    }
   ],
   "source": [
    "data = np.asarray([each['words'] for each in modeling_data['data']])\n",
    "label = np.asarray([each['hashtag_label'] for each in modeling_data['data']])\n",
    "\n",
    "print(data[:2])\n",
    "print(label[:2])\n",
    "print(len(data))\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flat_labels(labels):\n",
    "    flatted = []\n",
    "    l = modeling_data['categorical_num']\n",
    "    for label in labels:\n",
    "        m = [0.] * l\n",
    "        for each in label:\n",
    "            m = list(map(lambda x: x[0] or x[1], zip(m, each)))\n",
    "        flatted.append(m)\n",
    "    return np.asarray(flatted)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "categorical_label = list(map(lambda x: to_categorical(x, num_classes=modeling_data['categorical_num']), label))\n",
    "\n",
    "categorical_label_flatted = flat_labels(categorical_label)\n",
    "\n",
    "print(len(categorical_label))\n",
    "categorical_label_flatted[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_0, X_test, y_train_0, y_test = train_test_split(data, categorical_label_flatted, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ list(['rt', ':', 'new', 'infographic', 'on', 'how', 'most', 'cases', 'of', 'cervicalcancer', 'can', 'be', 'prevented', 'w', '/', 'tests', '&', 'hpv', 'vaccine', '.', 'vitalsigns'])\n",
      " list(['check', 'out', 'the', 'gci', 'team', \"'s\", 'newest', 'publication', 'on', 'hpv', 'vaccine', 'implementation', 'for', 'cancer', 'prevention', 'in', 'latinamerica', '!'])]\n",
      "[ list(['two', 'uk', 'girls', 'left', 'paralyzed', 'after', 'hpv', 'jabs', '.', 'authorities', 'still', 'claim', 'it', \"'s\", 'coincidence', '.'])\n",
      " list(['cervicalcancer', 'deaths', 'have', 'decreased', 'dramatically', 'over', 'the', 'past', '40', 'years', ',', 'mostly', 'due', 'to', 'increased', 'screening', '.'])]\n",
      "[[0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0]]\n",
      "[[0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_0[:2])\n",
    "print(X_test[:2])\n",
    "print(y_train_0[:2])\n",
    "print(y_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save 5% train samples to predication task\n",
    "cut = int(len(X_train_0) * 0.95)\n",
    "X_train = X_train_0[:cut]\n",
    "y_train = y_train_0[:cut]\n",
    "X_pred = X_train_0[cut:]\n",
    "y_pred = y_train_0[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-48877fc935ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfrequency\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mall_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodeling_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-48877fc935ac>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfrequency\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mall_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodeling_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!!!!!do not run this, run the third one below this to directly load the dictionary\n",
    "#create words dictionary for the data\n",
    "from functools import reduce\n",
    "\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "all_tokens = list(reduce(lambda x, y: x + y, [l['words'] for l in modeling_data['data']]))\n",
    "print(all_tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(all_tokens))\n",
    "\n",
    "for token in all_tokens:\n",
    "    frequency[token] += 1\n",
    "    \n",
    "dictionary = sorted(frequency.items(), key=lambda x:x[1], reverse=True)\n",
    "dictionary = [k for k,v in dictionary[:vocab_size]]\n",
    "print(dictionary[:10])\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dict('model/dict.dd', dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':',\n",
       " 'hpv',\n",
       " 'rt',\n",
       " 'vaccine',\n",
       " '.',\n",
       " 'the',\n",
       " ',',\n",
       " 'of',\n",
       " 'to',\n",
       " 'cancer',\n",
       " 'in',\n",
       " 'gardasil',\n",
       " 'for',\n",
       " 'cervicalcancer',\n",
       " 'a',\n",
       " '&',\n",
       " 'and',\n",
       " 'is',\n",
       " '!',\n",
       " 'cervical']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 0\n",
    "dictionary = load_dict('model/dict.dd')\n",
    "if (not vocab_size):\n",
    "    vocab_size = len(dictionary)\n",
    "dictionary[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word to index\n",
    "X_train_f = [[dictionary.index(word) if word in dictionary else -1 for word in doc] for doc in X_train]\n",
    "X_test_f = [[dictionary.index(word) if word in dictionary else -1 for word in doc] for doc in X_test]\n",
    "X_pred_f = [[dictionary.index(word) if word in dictionary else -1 for word in doc] for doc in X_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 0, 46, 781, 22, 74, 100, 121, 7, 13, 35, 55, 352, 54, 40, 162, 15, 1, 3, 4, 1126], [364, 93, 5, 10430, 1019, 20, 1770, 3765, 22, 1, 3, 1637, 12, 9, 61, 10, 8726, 18], [1159, 49, 251, 22, 97, 4382, 12, 448, 3863, 8, 20, 1, 25, 493, 18, 400, 13], [2, 0, 122, 312, 90, 662, 57, 8, 369, 33, 13, 4, 529, 26, 967, 16, 1777, 831, 8, 39, 365, 12, 1], [2, 0, 27, 0, 11, 131, 19, 328, 10, 31, 244, 54, 40, 1, 51]]\n",
      "[[210, 338, 41, 215, 165, 34, 1, 471, 4, 459, 182, 409, 29, 20, 411, 4], [13, 180, 57, 1813, 2238, 296, 5, 1477, 1003, 134, 6, 3875, 616, 8, 448, 67, 4], [2, 0, 27, 0, 14, 262, 7, 346, 362, 10, 14, 509, 110, 118, 250, 34, 168, 1, 3], [726, 12, 8803, 423, 23, 155, 15, 281, 223, 397, 8, 1, 0, 492, 287], [97, 1, 16, 9, 153, 2965, 3326, 3551, 22, 2585, 193, 18, 1077, 125, 8, 159, 0]]\n",
      "[[3, 1413, 264, 41, 177, 1, 68, 32, 757, 37, 697], [2, 0, 4, 84, 981, 18, 11, 3379, 6, 226, 2928, 10352, 1760, 81, 10353, 6, 4817, 19993, 4], [2, 0, 30, 24, 478, 11, 11792, 21], [1, 25, 68, 569, 145, 917, 41, 10, 2948, 1620, 45], [2, 0, 1026, 0, 2602, 1592, 16, 1282, 4158, 7, 19, 9, 422, 33, 1780]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_f[:5])\n",
    "print(X_test_f[:5])\n",
    "print(X_pred_f[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding words using GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 2, 17.012127213987693)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array([len(doc) for doc in X_train_f])\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 39\n",
    "embedding_dim = 200\n",
    "vecs, words, wordidx = load_vectors('twitter.27B.%dd'%(embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_u = sequence.pad_sequences(X_train_f, maxlen=seq_len)\n",
    "X_test_u = sequence.pad_sequences(X_test_f, maxlen=seq_len)\n",
    "X_pred_u = sequence.pad_sequences(X_pred_f, maxlen=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     2     0    46   781    22    74\n",
      "    100   121     7    13    35    55   352    54    40   162    15     1\n",
      "      3     4  1126]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0   364    93     5\n",
      "  10430  1019    20  1770  3765    22     1     3  1637    12     9    61\n",
      "     10  8726    18]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0  1159    49\n",
      "    251    22    97  4382    12   448  3863     8    20     1    25   493\n",
      "     18   400    13]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     2     0   122   312    90   662    57     8\n",
      "    369    33    13     4   529    26   967    16  1777   831     8    39\n",
      "    365    12     1]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      2     0    27     0    11   131    19   328    10    31   244    54\n",
      "     40     1    51]]\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0  210  338   41  215  165\n",
      "    34    1  471    4  459  182  409   29   20  411    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0   13  180   57 1813 2238  296\n",
      "     5 1477 1003  134    6 3875  616    8  448   67    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    2    0   27    0   14  262    7  346\n",
      "   362   10   14  509  110  118  250   34  168    1    3]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0  726   12 8803  423\n",
      "    23  155   15  281  223  397    8    1    0  492  287]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0   97    1   16    9  153 2965\n",
      "  3326 3551   22 2585  193   18 1077  125    8  159    0]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     3  1413   264    41   177     1    68    32\n",
      "    757    37   697]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     2     0     4    84\n",
      "    981    18    11  3379     6   226  2928 10352  1760    81 10353     6\n",
      "   4817 19993     4]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     2     0    30    24   478\n",
      "     11 11792    21]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     1    25    68   569   145   917    41    10\n",
      "   2948  1620    45]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      2     0  1026     0  2602  1592    16  1282  4158     7    19     9\n",
      "    422    33  1780]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_u[:5])\n",
    "print(X_test_u[:5])\n",
    "print(X_pred_u[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embedding(dictionary):\n",
    "    print(vecs.shape)\n",
    "    n_fact = vecs.shape[1]\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "\n",
    "    for i, word in enumerate(dictionary):\n",
    "        #if word:# and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
    "        src_idx = wordidx[word] if word in wordidx else 0\n",
    "        \n",
    "        if src_idx:\n",
    "            emb[i] = vecs[src_idx]\n",
    "        else:\n",
    "            # If we can't find the word in glove, randomly initialize\n",
    "            emb[i] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "\n",
    "    # This is our \"rare word\" id - we want to randomly initialize\n",
    "    emb[-1] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "    emb/=3\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1193517, 200)\n"
     ]
    }
   ],
   "source": [
    "embedding = create_embedding(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create NN mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 39, 200)           4661400   \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 39, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 39, 200)           200200    \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 39, 200)           200200    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 39, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 39, 200)           200200    \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 39, 200)           200200    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 39, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 39, 200)           200200    \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 39, 200)           200200    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 39, 200)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 7800)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 11)                85811     \n",
      "=================================================================\n",
      "Total params: 5,948,411\n",
      "Trainable params: 1,287,011\n",
      "Non-trainable params: 4,661,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#pure cnn model\n",
    "dth = 0.5\n",
    "filter_num = 5\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#embedding layer\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=seq_len, weights=[embedding], trainable=False))\n",
    "# model.add(Dropout(dth))\n",
    "\n",
    "#hidden layer\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Dropout(dth))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Dropout(dth))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Dropout(dth))\n",
    "\n",
    "#output layer\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(dth))\n",
    "model.add(Dense(modeling_data['categorical_num'], activation='softmax'))\n",
    "\n",
    "#compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 39, 200)           4661400   \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 39, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 39, 200)           200200    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 400)               641600    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 11)                4411      \n",
      "=================================================================\n",
      "Total params: 5,507,611\n",
      "Trainable params: 846,211\n",
      "Non-trainable params: 4,661,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Bi-LSTM model\n",
    "dth = 0.5\n",
    "filter_num = 5\n",
    "model = Sequential()\n",
    "\n",
    "#embedding layer\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=seq_len, weights=[embedding], trainable=False))\n",
    "model.add(Dropout(dth))\n",
    "\n",
    "#hidden layers\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Bidirectional(LSTM(embedding_dim)))\n",
    "# model.add(Bidirectional(LSTM(embedding_dim,  batch_input_shape=(64, 39, 50), stateful=True)))\n",
    "model.add(Dropout(dth))\n",
    "\n",
    "#output layer\n",
    "model.add(Dense(modeling_data['categorical_num'], activation='softmax'))\n",
    "\n",
    "#compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=1e-4\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "model_chk_path = 'model/cnn_hashtagcls_emb{}_weights.validation.h5'.format(embedding_dim)\n",
    "mcp = ModelCheckpoint(model_chk_path, monitor=\"val_loss\", verbose=1,\n",
    "                      save_best_only=True, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2939,\n",
       " 1: 2368,\n",
       " 2: 14558,\n",
       " 3: 2724,\n",
       " 4: 12978,\n",
       " 5: 20715,\n",
       " 6: 13457,\n",
       " 7: 2458,\n",
       " 8: 3738,\n",
       " 9: 52454,\n",
       " 10: 2196}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#class_weight\n",
    "cw_map = {'cancer': 4,\n",
    " 'cervicalcanc': 6,\n",
    " 'gardasil': 2,\n",
    " 'health': 0,\n",
    " 'hpv': 9,\n",
    " 'hpvvaccin': 3,\n",
    " 'learntherisk': 8,\n",
    " 'studi': 7,\n",
    " 'vaccin': 5,\n",
    " 'vaccineswork': 10,\n",
    " 'vax': 1}\n",
    "\n",
    "cw = [('hpv', 52454),\n",
    " ('vaccin', 20715),\n",
    " ('gardasil', 14558),\n",
    " ('cervicalcanc', 13457),\n",
    " ('cancer', 12978),\n",
    " ('learntherisk', 3738),\n",
    " ('health', 2939),\n",
    " ('hpvvaccin', 2724),\n",
    " ('studi', 2458),\n",
    " ('vax', 2368),\n",
    " ('vaccineswork', 2196)]\n",
    "\n",
    "class_weight = dict()\n",
    "\n",
    "for each in cw :\n",
    "    class_weight[cw_map[each[0]]] = each[1]\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61597 samples, validate on 16210 samples\n",
      "Epoch 1/500\n",
      "61597/61597 [==============================] - 714s 12ms/step - loss: 2.0325 - acc: 0.7104 - val_loss: 1.4967 - val_acc: 0.7469\n",
      "Epoch 2/500\n",
      "61597/61597 [==============================] - 715s 12ms/step - loss: 1.4997 - acc: 0.7396 - val_loss: 1.3689 - val_acc: 0.7266\n",
      "Epoch 3/500\n",
      "61597/61597 [==============================] - 717s 12ms/step - loss: 1.4019 - acc: 0.7478 - val_loss: 1.3027 - val_acc: 0.7387\n",
      "Epoch 4/500\n",
      "61597/61597 [==============================] - 713s 12ms/step - loss: 1.3634 - acc: 0.7535 - val_loss: 1.2923 - val_acc: 0.7307\n",
      "Epoch 5/500\n",
      "61597/61597 [==============================] - 711s 12ms/step - loss: 1.3374 - acc: 0.7562 - val_loss: 1.2825 - val_acc: 0.7548\n",
      "Epoch 6/500\n",
      "61597/61597 [==============================] - 711s 12ms/step - loss: 1.3226 - acc: 0.7559 - val_loss: 1.2578 - val_acc: 0.7854\n",
      "Epoch 7/500\n",
      "61597/61597 [==============================] - 710s 12ms/step - loss: 1.3091 - acc: 0.7570 - val_loss: 1.2560 - val_acc: 0.7187\n",
      "Epoch 8/500\n",
      "61597/61597 [==============================] - 714s 12ms/step - loss: 1.3029 - acc: 0.7606 - val_loss: 1.2450 - val_acc: 0.7209\n",
      "Epoch 9/500\n",
      "61597/61597 [==============================] - 715s 12ms/step - loss: 1.3005 - acc: 0.7613 - val_loss: 1.2401 - val_acc: 0.7992\n",
      "Epoch 10/500\n",
      "61597/61597 [==============================] - 713s 12ms/step - loss: 1.2898 - acc: 0.7603 - val_loss: 1.2324 - val_acc: 0.7957\n",
      "Epoch 11/500\n",
      "61597/61597 [==============================] - 712s 12ms/step - loss: 1.2868 - acc: 0.7600 - val_loss: 1.2329 - val_acc: 0.7283\n",
      "Epoch 12/500\n",
      "61597/61597 [==============================] - 712s 12ms/step - loss: 1.2850 - acc: 0.7620 - val_loss: 1.2431 - val_acc: 0.7718\n",
      "Epoch 13/500\n",
      "61597/61597 [==============================] - 713s 12ms/step - loss: 1.2836 - acc: 0.7632 - val_loss: 1.2322 - val_acc: 0.7394\n",
      "Epoch 14/500\n",
      "61597/61597 [==============================] - 714s 12ms/step - loss: 1.2802 - acc: 0.7641 - val_loss: 1.2278 - val_acc: 0.7374\n",
      "Epoch 15/500\n",
      "61597/61597 [==============================] - 714s 12ms/step - loss: 1.2786 - acc: 0.7628 - val_loss: 1.2288 - val_acc: 0.7429\n",
      "Epoch 16/500\n",
      "61597/61597 [==============================] - 711s 12ms/step - loss: 1.2818 - acc: 0.7642 - val_loss: 1.2283 - val_acc: 0.7981\n",
      "Epoch 17/500\n",
      "61597/61597 [==============================] - 716s 12ms/step - loss: 1.2774 - acc: 0.7656 - val_loss: 1.2242 - val_acc: 0.7381\n",
      "Epoch 18/500\n",
      "61597/61597 [==============================] - 715s 12ms/step - loss: 1.2770 - acc: 0.7642 - val_loss: 1.2210 - val_acc: 0.7052\n",
      "Epoch 19/500\n",
      "61597/61597 [==============================] - 716s 12ms/step - loss: 1.2757 - acc: 0.7641 - val_loss: 1.2223 - val_acc: 0.7652\n",
      "Epoch 20/500\n",
      "61597/61597 [==============================] - 717s 12ms/step - loss: 1.2747 - acc: 0.7650 - val_loss: 1.2227 - val_acc: 0.7579\n",
      "Epoch 21/500\n",
      "61597/61597 [==============================] - 715s 12ms/step - loss: 1.2764 - acc: 0.7635 - val_loss: 1.2212 - val_acc: 0.7429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x228143378d0>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick test\n",
    "model.fit(X_train_u, y_train, validation_data=(X_test_u, y_test), class_weight=None, epochs=500, batch_size=64, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### best result\n",
    "- 0.8353"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### current best model snapshot:\n",
    "\n",
    "```python\n",
    "dth = 0.5\n",
    "filter_num = 5\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#embedding layer\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=seq_len, weights=[embedding], trainable=False))\n",
    "\n",
    "#hidden layer\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Dropout(dth))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Dropout(dth))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Conv1D(embedding_dim, filter_num, padding='same', activation='relu'))\n",
    "model.add(Dropout(dth))\n",
    "\n",
    "#output layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(modeling_data['categorical_num'], activation='sigmoid'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=True\n",
    "model.optimizer.lr=1e-4\n",
    "model.fit(X_train_u, y_train, validation_data=(X_test_u, y_test), epochs=100, batch_size=64, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and save model for future usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the model\n",
    "model.save_weights('model/cnn_hashtagcls_emb{}_weights.h5'.format(embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine train and test data and retrain the model as the final model to use for predication\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5)\n",
    "model.fit(\n",
    "    np.concatenate((X_train_u, y_train), axis=0),\n",
    "    np.concatenate((X_test_u, y_test), axis=0),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping])\n",
    "model.save_weights(\n",
    "    \"model/all_cnn_hashtagcls_emb{}_weights.h5\".format(embedding_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract the last hidden layer information\n",
    ">https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Dense.__dir__>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predication\n",
    "**The predication will be performed on both no_labeled data set and neg_sample and pred set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load two data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "with open(\"temp/neg_sample_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    neg_sample = json.load(f)\n",
    "    \n",
    "with open(\"temp/no_labeled_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    nolabel_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...,   757    37   697]\n",
      " [    0     0     0 ...,  4817 19993     4]\n",
      " [    0     0     0 ...,    11 11792    21]\n",
      " ..., \n",
      " [    0     0     0 ...,     5     1     3]\n",
      " [    0     0     0 ...,   164    87    75]\n",
      " [    0     0     0 ...,    37  1632     0]]\n",
      "3242\n",
      "3242\n",
      "3242\n"
     ]
    }
   ],
   "source": [
    "print(X_pred_u)\n",
    "print(len(X_pred_u))\n",
    "print(len(y_pred))\n",
    "print(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model. predict_classes(X_pred_u)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
